{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib torchtext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /home/lee/anaconda3/lib/python3.6/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/lee/anaconda3/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "    /home/lee/anaconda3/lib/python3.6/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n",
      "Collecting de_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz#egg=de_core_news_sm==2.0.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz (38.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 38.2MB 1.0MB/s ta 0:00:0101 15% |████▉                           | 5.8MB 1.2MB/s eta 0:00:28\n",
      "\u001b[?25hInstalling collected packages: de-core-news-sm\n",
      "  Running setup.py install for de-core-news-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed de-core-news-sm-2.0.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/lee/anaconda3/lib/python3.6/site-packages/de_core_news_sm -->\n",
      "    /home/lee/anaconda3/lib/python3.6/site-packages/spacy/data/de\n",
      "\n",
      "    You can now load the model via spacy.load('de')\n",
      "\n",
      "downloading de-en.tgz\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.de.xml\n",
      ".data/iwslt/de-en/train.tags.de-en.en\n",
      ".data/iwslt/de-en/train.tags.de-en.de\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<bos>', '<eos>', ',', '.', 'die']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard PyTorch imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# For plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#!conda install torchtext spacy\n",
    "!python -m spacy download en\n",
    "!python -m spacy download de\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "url = re.compile('(<url>.*</url>)')\n",
    "\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "\n",
    "# Testing IWSLT\n",
    "DE = data.Field(tokenize=tokenize_de, init_token='<bos>', eos_token='<eos>', include_lengths=True)\n",
    "EN = data.Field(tokenize=tokenize_en, init_token='<bos>', eos_token='<eos>', include_lengths=True)\n",
    "\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN))\n",
    "\n",
    "\n",
    "train_it = data.Iterator(train, batch_size=4, sort_within_batch=True, train=True, repeat=False, shuffle=True)\n",
    "MIN_WORD_FREQ = 10\n",
    "MAX_NUM_WORDS = 1000\n",
    "DE.build_vocab(train.src, min_freq=MIN_WORD_FREQ, max_size=MAX_NUM_WORDS)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_WORD_FREQ, max_size=MAX_NUM_WORDS)\n",
    "\n",
    "DE.vocab.itos[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1004"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DE.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.ones((5, 4, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xflat = tf.reshape(X, (-1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(20, 3) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xflat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xflat.shape[-1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, length, ndims = [v.value for v in X.shape]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v = [tf.layers.dense(X, 3) for _ in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_expanded = tf.expand_dims(q, 1)\n",
    "k_expanded = tf.expand_dims(k, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(5), Dimension(1), Dimension(4), Dimension(3)]),\n",
       " TensorShape([Dimension(5), Dimension(4), Dimension(1), Dimension(3)]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_expanded.shape, k_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(5), Dimension(4), Dimension(4)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(q_expanded * k_expanded, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_raw = tf.reduce_sum(q_expanded * k_expanded, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.expand_dims(tf.nn.softmax(s_raw, 1), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v_expanded = tf.expand_dims(v, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(5), Dimension(1), Dimension(4), Dimension(3)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(5), Dimension(4), Dimension(4), Dimension(1)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.reduce_sum(v_expanded * s, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(5), Dimension(4), Dimension(3)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Max:0' shape=(5,) dtype=float32>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_max(a, (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "tf.reset_default_graph()\n",
    "from nn_utils import *\n",
    "\n",
    "def masked_softmax(v, mask, dim=1):\n",
    "    v_masked = v * mask\n",
    "    v_max = tf.reshape(tf.reduce_max(v_masked, (1, 2)), (-1, 1, 1))\n",
    "    v_stable = v_masked - v_max\n",
    "    v_exp = tf.exp(v_stable)\n",
    "    v_exp_masked = v_exp * mask\n",
    "    v_exp_summed = tf.expand_dims(tf.reduce_sum(v_exp_masked, dim), dim)\n",
    "    \n",
    "    return v_exp_masked / v_exp_summed\n",
    "    \n",
    "    \n",
    "class AttentionLayer:\n",
    "    def __init__(self, X, mask, X_decode = None, decode_mask = None, ff_layer = True):\n",
    "        #If X_decode is not none, this is the decoder module that takes in two embeddings\n",
    "        #Otherwise, this is the standard self-attention layer\n",
    "        bs, length, ndims = [v.value for v in X.shape]\n",
    "        if X_decode is None:\n",
    "            self.q, self.k, self.v = [tf.tanh(tf.layers.dense(X, ndims)) for _ in range(3)]\n",
    "        else:\n",
    "            self.k, self.v = [tf.tanh(tf.layers.dense(X, ndims)) for _ in range(2)]\n",
    "            self.q = tf.tanh(tf.layers.dense(X_decode, ndims))\n",
    "        #dimensions are batch, attn head, attn tail, emb\n",
    "        self.q_expanded = tf.expand_dims(self.q, 1)\n",
    "        self.k_expanded = tf.expand_dims(self.k, 2)\n",
    "        self.s_raw = tf.reduce_sum(self.q_expanded * self.k_expanded, -1)\n",
    "        enc_mask = tf.expand_dims(mask, 1)\n",
    "        if decode_mask is None:\n",
    "            dec_mask = tf.expand_dims(mask, 2)\n",
    "        else:\n",
    "            dec_mask = tf.expand_dims(decode_mask, 2)\n",
    "        self.combined_mask = combined_mask = enc_mask * dec_mask\n",
    "        self.s = masked_softmax(self.s_raw, combined_mask, dim=1)\n",
    "        self.v_expanded = tf.expand_dims(self.v, 1)\n",
    "        self.a = tf.reduce_sum(self.v_expanded * self.s, 2)\n",
    "        if X_decode is None:\n",
    "            residual_identity = X\n",
    "        else:\n",
    "            residual_identity = X_decode\n",
    "        self.e_raw = self.a + residual_identity\n",
    "        self.e = layer_norm(self.e_raw)\n",
    "        if ff_layer:\n",
    "            self.e_tilde_raw = tf.layers.dense(X, X.shape[-1].value) + X\n",
    "            self.e_tilde = layer_norm(self.e_tilde_raw)\n",
    "            self.output = self.e_tilde\n",
    "        else:\n",
    "            self.output = self.e\n",
    "class Encoder:\n",
    "    def __init__(self, num_wds, wd_ind, mask, ndims = 20, n_layers = 2):\n",
    "        self.wd_ind = wd_ind\n",
    "        self.num_wds = num_wds\n",
    "        self.mask = mask\n",
    "        self.wd_emb = tf.Variable(\n",
    "            tf.random_uniform([self.num_wds, ndims],minval = -1, maxval = 1.))\n",
    "        self.length = tf.shape(self.wd_ind)[1]\n",
    "        self.wd_vec = tf.nn.embedding_lookup(self.wd_emb, wd_ind)\n",
    "        self.position = tf.reshape(tf.range(tf.cast(self.length, tf.float32), dtype=tf.float32)/10000, (1, -1, 1))\n",
    "        \n",
    "        # for debugging purposes\n",
    "        self.encodings = []\n",
    "        #self.attn_layers = []\n",
    "        last_encoding = self.wd_vec + self.position\n",
    "        for _ in range(n_layers):\n",
    "            attn_layer = AttentionLayer(last_encoding, mask)\n",
    "            last_encoding = attn_layer.output\n",
    "            #last_encoding = LinearResNorm(attn_layer.e)\n",
    "            #self.attn_layers.append(attn_layer)\n",
    "            self.encodings.append(last_encoding)\n",
    "        \n",
    "        \n",
    "class Decoder:\n",
    "    def __init__(self, num_wds, wd_ind, mask, encoder, ndims = 20, n_layers = 2):\n",
    "        self.encoder = encoder\n",
    "        self.encodings = encoder.encodings\n",
    "        input_mask = encoder.mask\n",
    "        self.num_wds = num_wds\n",
    "        self.wd_ind = wd_ind\n",
    "        self.length = tf.shape(self.wd_ind)[1]\n",
    "        self.num_wds = num_wds\n",
    "        self.wd_emb = tf.Variable(\n",
    "            tf.random_uniform([self.num_wds, ndims],minval = -1, maxval = 1.))\n",
    "        self.wd_vec = tf.nn.embedding_lookup(self.wd_emb, wd_ind)\n",
    "        self.position = tf.reshape(tf.range(tf.cast(self.length, tf.float32), dtype=tf.float32)/10000, (1, -1, 1))\n",
    "        last_encoding =last_encoding = self.wd_vec + self.position\n",
    "        self.self_attentions = []\n",
    "        self.second_attentions = []\n",
    "        for idx in range(n_layers):\n",
    "            encodings = self.encodings[idx]\n",
    "            self_attention = AttentionLayer(last_encoding, mask, ff_layer = False).output\n",
    "            second_attention = AttentionLayer(encodings, input_mask, X_decode = self_attention,\n",
    "                                             decode_mask = mask).output\n",
    "            last_encoding = second_attention\n",
    "            self.self_attentions.append(self_attention)\n",
    "            self.second_attentions.append(second_attention)\n",
    "            #linear_res_norm = LinearResNorm(attn_layer.e)\n",
    "            \n",
    "        self.presoftmax_output = tf.layers.dense(last_encoding, num_wds)\n",
    "        #self.output = tf.nn.softmax(self.presoftmax_output)\n",
    "        self.output = masked_softmax(self.presoftmax_output, mask, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "    def __init__(self, num_wds):\n",
    "        self.num_wds = num_wds\n",
    "        self.learning_rate = tf.placeholder(tf.float32, ())\n",
    "        self.wd_ind_src = wd_ind_src = tf.placeholder(tf.int32, (None, None))\n",
    "        self.wd_ind_trg = wd_ind_trg = tf.placeholder(tf.int32, (None, None))\n",
    "        self.input_lengths = tf.placeholder(tf.int32, [None])\n",
    "        self.output_lengths = tf.placeholder(tf.int32, [None])\n",
    "        self.input_mask = tf.sequence_mask(\n",
    "            self.input_lengths, maxlen = tf.shape(self.wd_ind_src)[-1], dtype = tf.float32)\n",
    "        self.output_mask = tf.sequence_mask(\n",
    "            self.output_lengths, maxlen = tf.shape(self.wd_ind_trg)[-1], dtype = tf.float32)\n",
    "        self.encoder = Encoder(num_wds, wd_ind_src, self.input_mask)\n",
    "        self.decoder = Decoder(num_wds, wd_ind_trg, self.output_mask, self.encoder)\n",
    "        self.presoftmax_output = self.decoder.presoftmax_output\n",
    "        self.output = self.decoder.output\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels = self.wd_ind_trg, logits = self.presoftmax_output)\n",
    "        self.optimizer, self.grad_norm_total = apply_clipped_optimizer(opt, self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transfomer = Transformer(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[1,0] = 538 is not in [0, 20)\n\t [[Node: embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Adam/update_Variable/ApplyAdam\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable/read, _arg_Placeholder_1_0_1, embedding_lookup/axis)]]\n\nCaused by op 'embedding_lookup', defined at:\n  File \"/home/lee/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/lee/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 497, in start\n    self.io_loop.start()\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/lee/anaconda3/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/lee/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/home/lee/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-55-7a9e2882b548>\", line 1, in <module>\n    transfomer = Transformer(20)\n  File \"<ipython-input-54-7216ede6060a>\", line 118, in __init__\n    self.encoder = Encoder(num_wds, wd_ind_src, self.input_mask)\n  File \"<ipython-input-54-7216ede6060a>\", line 59, in __init__\n    self.wd_vec = tf.nn.embedding_lookup(self.wd_emb, wd_ind)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 310, in embedding_lookup\n    transform_fn=None)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 133, in _embedding_lookup_and_transform\n    result = _clip(array_ops.gather(params[0], ids, name=name),\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 2659, in gather\n    return gen_array_ops.gather_v2(params, indices, axis, name=name)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3142, in gather_v2\n    \"GatherV2\", params=params, indices=indices, axis=axis, name=name)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[1,0] = 538 is not in [0, 20)\n\t [[Node: embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Adam/update_Variable/ApplyAdam\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable/read, _arg_Placeholder_1_0_1, embedding_lookup/axis)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[1,0] = 538 is not in [0, 20)\n\t [[Node: embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Adam/update_Variable/ApplyAdam\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable/read, _arg_Placeholder_1_0_1, embedding_lookup/axis)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-a450ca31da86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mtransfomer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd_ind_trg\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtrg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransfomer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_lengths\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtrg_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     transfomer.learning_rate : 1e-2}\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransfomer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransfomer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[1,0] = 538 is not in [0, 20)\n\t [[Node: embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Adam/update_Variable/ApplyAdam\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable/read, _arg_Placeholder_1_0_1, embedding_lookup/axis)]]\n\nCaused by op 'embedding_lookup', defined at:\n  File \"/home/lee/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/lee/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 497, in start\n    self.io_loop.start()\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/lee/anaconda3/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/lee/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/home/lee/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-55-7a9e2882b548>\", line 1, in <module>\n    transfomer = Transformer(20)\n  File \"<ipython-input-54-7216ede6060a>\", line 118, in __init__\n    self.encoder = Encoder(num_wds, wd_ind_src, self.input_mask)\n  File \"<ipython-input-54-7216ede6060a>\", line 59, in __init__\n    self.wd_vec = tf.nn.embedding_lookup(self.wd_emb, wd_ind)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 310, in embedding_lookup\n    transform_fn=None)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 133, in _embedding_lookup_and_transform\n    result = _clip(array_ops.gather(params[0], ids, name=name),\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 2659, in gather\n    return gen_array_ops.gather_v2(params, indices, axis, name=name)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3142, in gather_v2\n    \"GatherV2\", params=params, indices=indices, axis=axis, name=name)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/home/lee/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[1,0] = 538 is not in [0, 20)\n\t [[Node: embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Adam/update_Variable/ApplyAdam\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable/read, _arg_Placeholder_1_0_1, embedding_lookup/axis)]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for train_batch in train_it:\n",
    "    src_tensor  = train_batch.src[0].data.cpu().numpy()\n",
    "    src_len = train_batch.src[1].cpu().numpy()\n",
    "    trg_tensor  = train_batch.trg[0].data.cpu().numpy()\n",
    "    trg_len = train_batch.trg[1].cpu().numpy()\n",
    "#     print(src_tensor.shape, src_len.shape, trg_tensor.shape, trg_len.shape)\n",
    "#     print(src_tensor, src_len, trg_tensor, trg_len)\n",
    "    trn_feed_dict = {transfomer.wd_ind_src : src_tensor, transfomer.input_lengths : src_len,\n",
    "                    transfomer.wd_ind_trg : trg_tensor, transfomer.output_lengths : trg_len,\n",
    "                    transfomer.learning_rate : 1e-2}\n",
    "    _, loss = sess.run([transfomer.optimizer, transfomer.loss], trn_feed_dict)\n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 4 from IWSLT]\n",
       "\t[.src]:('[torch.LongTensor of size 29x4]', '[torch.LongTensor of size 4]')\n",
       "\t[.trg]:('[torch.LongTensor of size 37x4]', '[torch.LongTensor of size 4]')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch.src[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WMT14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WMT14('data/', ('.en', '.de'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load words from IWSLT\n",
    "\n",
    "#!pip install torchtext spacy\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download de\n",
    "from torchtext import data, datasets\n",
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "BLANK_WORD = \"<blank>\"\n",
    "SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)\n",
    "TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, \n",
    "                 eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
    "\n",
    "MAX_LEN = 100\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(SRC, TGT), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "MIN_FREQ = 1\n",
    "SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "TGT.build_vocab(train.trg, min_freq=MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib spacy torchtext seaborn "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
