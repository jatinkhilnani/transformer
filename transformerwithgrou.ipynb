{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib torchtext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1004, 1004)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard PyTorch imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# For plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#!conda install torchtext spacy\n",
    "# !python -m spacy download en\n",
    "# !python -m spacy download de\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "url = re.compile('(<url>.*</url>)')\n",
    "\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "\n",
    "# Testing IWSLT\n",
    "DE = data.Field(tokenize=tokenize_de, init_token='<bos>', eos_token='<eos>', include_lengths=True)\n",
    "EN = data.Field(tokenize=tokenize_en, init_token='<bos>', eos_token='<eos>', include_lengths=True)\n",
    "\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN))\n",
    "\n",
    "\n",
    "train_it = data.Iterator(train, batch_size=4, sort_within_batch=True, train=True, repeat=False, shuffle=True)\n",
    "MIN_WORD_FREQ = 10\n",
    "MAX_NUM_WORDS = 1000\n",
    "DE.build_vocab(train.src, min_freq=MIN_WORD_FREQ, max_size=MAX_NUM_WORDS)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_WORD_FREQ, max_size=MAX_NUM_WORDS)\n",
    "\n",
    "num_wds_input = len(DE.vocab.itos)\n",
    "num_wds_output = len(EN.vocab.itos)\n",
    "\n",
    "num_wds_input, num_wds_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.contrib.layers import layer_norm\n",
    "import nn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class masked_softmax:\n",
    "    def __init__(self, v, mask, dim=1):\n",
    "        #bs, query dimension, key dimension\n",
    "        v_mask = v * mask\n",
    "        v_max = tf.reduce_max(v_mask, dim, keep_dims=True)\n",
    "        v_stable = v_mask - v_max\n",
    "\n",
    "        v_exp = tf.exp(v_stable) * mask\n",
    "        v_exp_sum = tf.reduce_sum(v_exp, dim, keep_dims=True)\n",
    "        self.v_mask, self.v_max, self.v_stable, self.v_exp, self.v_exp_sum = \\\n",
    "            v_mask, v_max, v_stable, v_exp, v_exp_sum\n",
    "        self.output =  v_exp / (v_exp_sum + 1e-20)\n",
    "\n",
    "\n",
    "class Encoder:\n",
    "  def __init__(self, num_wds, wd_ind, mask, ndims=20, n_layers=1):\n",
    "    self.num_wds = num_wds\n",
    "    self.wd_ind = wd_ind\n",
    "    self.mask = mask\n",
    "    self.length = tf.shape(self.wd_ind)[1]\n",
    "    self.wd_emb = tf.Variable(\n",
    "        tf.random_uniform([self.num_wds, ndims], minval=-1, maxval=1.))\n",
    "    self.wd_vec = tf.nn.embedding_lookup(self.wd_emb, wd_ind)\n",
    "    self.position = tf.reshape(\n",
    "        tf.range(tf.cast(self.length, tf.float32), dtype=tf.float32) / 10000,\n",
    "        (1, -1, 1))\n",
    "    self.w_tilde = embedding = self.wd_vec + self.position\n",
    "    self.encoding = []\n",
    "    self.attentionLayers = []\n",
    "    for _ in range(n_layers):\n",
    "      attentionLayer = AttentionLayer(embedding, mask)\n",
    "      embedding = attentionLayer.output\n",
    "      self.encoding.append(embedding)\n",
    "      self.attentionLayers.append(attentionLayer)\n",
    "\n",
    "\n",
    "class AttentionLayer:\n",
    "  def __init__(self, X, mask, X_decode=None, decode_mask=None, ff_layer=True):\n",
    "    bs, length, ndim = [v.value for v in X.shape]\n",
    "    self.X = X\n",
    "    if X_decode is None:\n",
    "      self.q, self.k, self.v = [\n",
    "          tf.tanh(tf.layers.dense(X, ndim)) for _ in range(3)\n",
    "      ]\n",
    "      decode_mask = mask\n",
    "    else:\n",
    "      self.q = tf.tanh(tf.layers.dense(X_decode, ndim))\n",
    "      self.k, self.v = [tf.tanh(tf.layers.dense(X, ndim)) for _ in range(2)]\n",
    "    #batch, attention queries, attention keys, embeddings\n",
    "    self.q_expanded = tf.expand_dims(self.q, 2)\n",
    "    self.k_expanded = tf.expand_dims(self.k, 1)\n",
    "    self.v_expanded = tf.expand_dims(self.v, 1)\n",
    "    self.s_raw = tf.reduce_sum(self.q_expanded * self.k_expanded, -1)\n",
    "    self.mask = tf.expand_dims(decode_mask, 2) * tf.expand_dims(mask, 1)\n",
    "    self.masked_softmax = masked_softmax(self.s_raw, self.mask)\n",
    "    self.s = self.masked_softmax.output\n",
    "    self.a = tf.expand_dims(self.s * self.mask, -1) * self.v_expanded\n",
    "    #A is shape bs, query, key, emb\n",
    "    self.a_compressed = tf.reduce_sum(self.a, 2)\n",
    "    if X_decode is None:\n",
    "        self.e = layer_norm(self.a_compressed + X)\n",
    "    else:\n",
    "        self.e = layer_norm(self.a_compressed + X_decode)\n",
    "    if ff_layer:\n",
    "      self.output = layer_norm(tf.layers.dense(self.e, ndim) + self.e)\n",
    "    else:\n",
    "      self.output = self.e\n",
    "\n",
    "\n",
    "class Decoder:\n",
    "  def __init__(self, num_wds, wd_ind, mask, encoder, ndims=20, n_layers=1):\n",
    "    self.num_wds = num_wds\n",
    "    self.wd_ind = wd_ind\n",
    "    self.mask = mask\n",
    "    self.encoder = encoder\n",
    "    self.length = tf.shape(self.wd_ind)[1]\n",
    "    self.wd_emb = tf.Variable(\n",
    "        tf.random_uniform([self.num_wds, ndims], minval=-1, maxval=1.))\n",
    "    self.wd_vec = tf.nn.embedding_lookup(self.wd_emb, wd_ind)\n",
    "    self.position = tf.reshape(\n",
    "        tf.range(tf.cast(self.length, tf.float32), dtype=tf.float32) / 10000,\n",
    "        (1, -1, 1))\n",
    "    self.w_tilde = embedding = self.wd_vec + self.position\n",
    "    self.decoding = []\n",
    "    self.self_attentions = []\n",
    "    self.encoder_attentions = []\n",
    "    for l_idx in range(n_layers):\n",
    "      attn = AttentionLayer(embedding, mask, ff_layer=False)\n",
    "      self.self_attentions.append(attn)\n",
    "      encode_attn = AttentionLayer(encoder.encoding[l_idx], encoder.mask,\n",
    "                                   attn.output, mask)\n",
    "      self.encoder_attentions.append(encode_attn)\n",
    "      embedding = encode_attn.output\n",
    "\n",
    "    self.output_raw = tf.layers.dense(embedding, num_wds)\n",
    "    #bs, word in sentence of target, embedding\n",
    "    \n",
    "    self.masked_softmax = masked_softmax(self.output_raw, mask)\n",
    "    self.output = self.masked_softmax.output\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "  def __init__(self, num_wds):\n",
    "    self.num_wds = num_wds\n",
    "    self.learning_rate = tf.placeholder(tf.float32, None)\n",
    "    self.wd_ind_src = wd_ind_src = tf.placeholder(tf.int32, (None, None))\n",
    "    self.wd_ind_trg = wd_ind_trg = tf.placeholder(tf.int32, (None, None))\n",
    "    self.input_lengths = tf.placeholder(tf.int32, [None])\n",
    "    self.output_lengths = tf.placeholder(tf.int32, [None])\n",
    "    self.input_mask = tf.sequence_mask(\n",
    "        self.input_lengths,\n",
    "        maxlen=tf.shape(self.wd_ind_src)[-1],\n",
    "        dtype=tf.float32)\n",
    "    self.output_mask = tf.sequence_mask(\n",
    "        self.output_lengths,\n",
    "        maxlen=tf.shape(self.wd_ind_trg)[-1],\n",
    "        dtype=tf.float32)\n",
    "    self.encoder = Encoder(num_wds, wd_ind_src, self.input_mask)\n",
    "    self.decoder = Decoder(num_wds, wd_ind_trg, self.output_mask, self.encoder)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "    self.prediction_mask = tf.concat((tf.zeros((4, 1)), self.output_mask[:,:-1] - self.output_mask[:,1:]), 1)\n",
    "    self.loss = tf.reduce_mean(tf.reduce_max(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=self.wd_ind_trg, logits=self.decoder.output_raw) * self.prediction_mask\n",
    "      , 1))\n",
    "    self.optimizer, self.grad_norm_total = nn_utils.apply_clipped_optimizer(\n",
    "        opt, self.loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.860581\n",
      "6.8824887\n",
      "6.848736\n",
      "6.8816223\n",
      "6.452512\n",
      "6.4300604\n",
      "6.627413\n",
      "6.5195527\n",
      "6.6538987\n",
      "6.570581\n",
      "6.151683\n",
      "5.1754093\n",
      "5.641946\n",
      "6.77978\n",
      "3.501072\n",
      "6.6031303\n",
      "4.5910125\n",
      "6.240979\n",
      "6.0886965\n",
      "6.618112\n",
      "5.8758736\n",
      "4.112165\n",
      "5.871326\n",
      "4.4997735\n",
      "3.837925\n",
      "7.826671\n",
      "7.142421\n",
      "3.970683\n",
      "6.320285\n",
      "6.3783855\n",
      "4.25441\n",
      "7.2489214\n",
      "0.8430574\n",
      "6.6169786\n",
      "7.1854715\n",
      "3.38065\n",
      "4.4561653\n",
      "4.8446665\n",
      "4.2707424\n",
      "4.2078886\n",
      "3.0726619\n",
      "6.6337357\n",
      "2.455357\n",
      "6.7401123\n",
      "7.748409\n",
      "3.767892\n",
      "5.216974\n",
      "6.6054926\n",
      "2.642487\n",
      "5.8652964\n",
      "3.8745844\n",
      "3.2137039\n",
      "5.069334\n",
      "5.487368\n",
      "3.9541762\n",
      "3.7334604\n",
      "5.7096324\n",
      "2.7235742\n",
      "5.393903\n",
      "6.703521\n",
      "5.0366616\n",
      "4.7399964\n",
      "4.2854013\n",
      "2.4378104\n",
      "3.316927\n",
      "2.82351\n",
      "6.2732267\n",
      "2.7403042\n",
      "4.5424943\n",
      "3.274647\n",
      "3.0037863\n",
      "5.1188736\n",
      "3.0483043\n",
      "4.6863527\n",
      "4.045103\n",
      "5.707782\n",
      "1.4244723\n",
      "6.1881843\n",
      "3.3271418\n",
      "3.8717227\n",
      "5.4050965\n",
      "3.4128418\n",
      "1.3608876\n",
      "0.69700176\n",
      "2.3570175\n",
      "5.038023\n",
      "1.9138532\n",
      "2.313255\n",
      "3.6385365\n",
      "4.1894193\n",
      "7.2299967\n",
      "5.0545893\n",
      "4.1882067\n",
      "5.243911\n",
      "5.8925\n",
      "2.3702826\n",
      "4.5857315\n",
      "4.4238787\n",
      "5.096237\n",
      "0.8655406\n",
      "2.8627644\n",
      "3.630424\n",
      "1.1195713\n",
      "2.9816017\n",
      "3.2113042\n",
      "4.7917023\n",
      "4.1809206\n",
      "2.3910675\n",
      "4.672701\n",
      "3.8275309\n",
      "0.81692016\n",
      "4.8151\n",
      "2.816266\n",
      "4.212493\n",
      "2.3731825\n",
      "0.8830422\n",
      "4.6440973\n",
      "2.3206525\n",
      "2.2786222\n",
      "2.749498\n",
      "4.089798\n",
      "4.9265194\n",
      "3.7781386\n",
      "4.4640207\n",
      "2.02121\n",
      "1.4079165\n",
      "3.9755816\n",
      "4.1188173\n",
      "2.5421119\n",
      "1.4453105\n",
      "3.4153488\n",
      "3.989151\n",
      "3.7245274\n",
      "3.3651757\n",
      "4.0945206\n",
      "2.1288905\n",
      "3.394955\n",
      "2.1604552\n",
      "1.1621279\n",
      "3.4515467\n",
      "1.5786017\n",
      "3.8169923\n",
      "5.316389\n",
      "3.519354\n",
      "3.2207723\n",
      "0.49393025\n",
      "3.5448692\n",
      "1.1271043\n",
      "4.983691\n",
      "1.1654716\n",
      "3.136743\n",
      "2.9848523\n",
      "0.43676594\n",
      "3.9447508\n",
      "4.7785873\n",
      "5.278085\n",
      "4.616536\n",
      "1.8062025\n",
      "3.55157\n",
      "4.368305\n",
      "2.0741863\n",
      "5.3658495\n",
      "4.512148\n",
      "3.709822\n",
      "1.1461341\n",
      "2.087662\n",
      "2.0603023\n",
      "7.7715163\n",
      "4.1107936\n",
      "2.1298316\n",
      "4.034824\n",
      "4.7680645\n",
      "2.7636495\n",
      "0.14221431\n",
      "3.0301769\n",
      "2.0544736\n",
      "6.000475\n",
      "2.760056\n",
      "2.4164596\n",
      "2.150288\n",
      "1.518747\n",
      "3.9481916\n",
      "2.8609693\n",
      "1.0976851\n",
      "3.4344068\n",
      "1.2825645\n",
      "3.4923525\n",
      "2.1100867\n",
      "2.7761073\n",
      "2.3749094\n",
      "0.5360006\n",
      "3.9731393\n",
      "3.4767458\n",
      "2.3455606\n",
      "2.980393\n",
      "1.2381417\n",
      "4.3162575\n",
      "3.7960906\n",
      "4.5778656\n",
      "2.3167796\n",
      "0.8435845\n",
      "3.6651118\n",
      "1.6323857\n",
      "3.261899\n",
      "4.0258427\n",
      "0.9843651\n",
      "2.3326764\n",
      "4.5816464\n",
      "2.4963377\n",
      "1.6185963\n",
      "0.83451605\n",
      "4.8314295\n",
      "2.7277513\n",
      "3.5255451\n",
      "4.496675\n",
      "6.239041\n",
      "2.8070066\n",
      "6.0375304\n",
      "2.5618594\n",
      "4.4006567\n",
      "0.7648743\n",
      "1.7527657\n",
      "2.7130494\n",
      "4.0213437\n",
      "0.1843284\n",
      "4.1943116\n",
      "1.3385122\n",
      "3.849444\n",
      "2.4572513\n",
      "1.3655072\n",
      "0.9310969\n",
      "2.1739893\n",
      "1.4319813\n",
      "6.6652246\n",
      "0.82501715\n",
      "4.2949233\n",
      "5.8992863\n",
      "3.4106715\n",
      "2.5100298\n",
      "1.4356004\n",
      "1.0567038\n",
      "3.9544406\n",
      "1.2233425\n",
      "1.9728465\n",
      "0.66575205\n",
      "2.2292194\n",
      "1.7210329\n",
      "3.2060542\n",
      "2.1991665\n",
      "1.0122377\n",
      "3.630229\n",
      "2.0068812\n",
      "2.3335464\n",
      "1.7855036\n",
      "2.4634745\n",
      "1.7517941\n",
      "1.7078067\n",
      "5.840028\n",
      "6.1804976\n",
      "5.5298796\n",
      "2.7441025\n",
      "3.473014\n",
      "1.3560805\n",
      "2.7181263\n",
      "3.2233398\n",
      "1.5573885\n",
      "2.9370518\n",
      "3.6312065\n",
      "3.8991702\n",
      "1.7321641\n",
      "4.5920877\n",
      "1.6375675\n",
      "3.647872\n",
      "4.804192\n",
      "2.558884\n",
      "6.0889335\n",
      "2.4334037\n",
      "2.0237226\n",
      "2.542122\n",
      "4.0106697\n",
      "3.4819036\n",
      "2.1827574\n",
      "3.8170466\n",
      "2.966318\n",
      "5.539547\n",
      "5.0692353\n",
      "3.3102252\n",
      "0.8716037\n",
      "2.8669615\n",
      "3.0973446\n",
      "3.3531601\n",
      "3.922641\n",
      "1.2018138\n",
      "4.6613917\n",
      "6.9915204\n",
      "3.8484092\n",
      "0.8777266\n",
      "1.7068168\n",
      "4.330902\n",
      "2.4347706\n",
      "2.4582558\n",
      "4.5077863\n",
      "3.6260395\n",
      "3.6965015\n",
      "4.453753\n",
      "2.7009642\n",
      "2.6941843\n",
      "0.3810314\n",
      "3.7126977\n",
      "4.621743\n",
      "1.4302313\n",
      "5.0568376\n",
      "4.1085024\n",
      "4.0498104\n",
      "6.468038\n",
      "1.6217636\n",
      "1.8085542\n",
      "2.035619\n",
      "5.8820105\n",
      "1.9801768\n",
      "1.0631592\n",
      "2.3893714\n",
      "0.9859798\n",
      "3.067752\n",
      "5.8334646\n",
      "1.3458427\n",
      "3.3981822\n",
      "1.8324456\n",
      "3.6168327\n",
      "1.9275917\n",
      "0.38746136\n",
      "2.291465\n",
      "5.0902348\n",
      "5.9692607\n",
      "3.988096\n",
      "1.4419286\n",
      "6.479238\n",
      "4.4800434\n",
      "5.2117634\n",
      "2.6604946\n",
      "4.2580185\n",
      "4.728423\n",
      "5.4477854\n",
      "4.705451\n",
      "3.633303\n",
      "2.4357734\n",
      "0.29407516\n",
      "1.5379689\n",
      "2.746068\n",
      "0.019559352\n",
      "5.4560504\n",
      "0.07293445\n",
      "1.1604474\n",
      "2.8905644\n",
      "1.2639525\n",
      "1.3350973\n",
      "1.8513566\n",
      "2.1541867\n",
      "3.0558567\n",
      "3.6508446\n",
      "4.514184\n",
      "0.9137479\n",
      "1.1612766\n",
      "3.3006537\n",
      "3.8901172\n",
      "2.5134847\n",
      "3.3652358\n",
      "0.3604438\n",
      "4.5156775\n",
      "1.8036888\n",
      "1.5825723\n",
      "3.4796588\n",
      "2.4630578\n",
      "3.3960094\n",
      "5.4282627\n",
      "3.515541\n",
      "2.755252\n",
      "3.1423013\n",
      "3.7526855\n",
      "3.3396277\n",
      "2.4010186\n",
      "2.0527134\n",
      "0.8959293\n",
      "2.488235\n",
      "2.5751367\n",
      "3.4536927\n",
      "1.5874089\n",
      "4.318337\n",
      "0.35994568\n",
      "1.8992996\n",
      "3.819461\n",
      "5.8366785\n",
      "3.6788192\n",
      "2.918274\n",
      "5.4743767\n",
      "0.350226\n",
      "1.4154518\n",
      "2.0854585\n",
      "2.8068612\n",
      "1.8229488\n",
      "1.9899197\n",
      "4.936941\n",
      "2.678701\n",
      "0.5413018\n",
      "1.9685866\n",
      "1.2085121\n",
      "5.61123\n",
      "0.009310058\n",
      "3.66411\n",
      "2.277122\n",
      "1.17232\n",
      "3.9406877\n",
      "4.6032753\n",
      "2.773127\n",
      "2.8859458\n",
      "2.289439\n",
      "0.0007047564\n",
      "0.70984113\n",
      "2.0623736\n",
      "3.7942705\n",
      "5.594753\n",
      "3.1756938\n",
      "2.45921\n",
      "6.4913254\n",
      "1.9565027\n",
      "3.5917206\n",
      "2.8131876\n",
      "2.129842\n",
      "4.5921755\n",
      "3.9230042\n",
      "3.0161283\n",
      "0.42481726\n",
      "4.5312095\n",
      "2.5849435\n",
      "3.6316051\n",
      "2.9068787\n",
      "4.853218\n",
      "4.2465897\n",
      "3.0852027\n",
      "2.4351563\n",
      "5.5384293\n",
      "2.5793242\n",
      "4.106528\n",
      "5.551235\n",
      "1.4273944\n",
      "1.267914\n",
      "1.1688055\n",
      "1.9728839\n",
      "1.9364033\n",
      "3.4102232\n",
      "4.2417526\n",
      "4.351469\n",
      "3.93884\n",
      "3.4328625\n",
      "4.336295\n",
      "4.2602654\n",
      "3.5276198\n",
      "2.008363\n",
      "4.173943\n",
      "0.18047157\n",
      "3.7564425\n",
      "4.5977335\n",
      "1.6002011\n",
      "3.9385502\n",
      "0.9010187\n",
      "2.5279002\n",
      "3.1139433\n",
      "2.1526818\n",
      "4.588852\n",
      "2.7741795\n",
      "1.766575\n",
      "2.3953424\n",
      "2.1918907\n",
      "2.9916024\n",
      "3.3477774\n",
      "2.2481532\n",
      "2.7225962\n",
      "1.1534815\n",
      "5.2366037\n",
      "0.76327527\n",
      "0.011839463\n",
      "5.8477125\n",
      "5.596543\n",
      "4.0552273\n",
      "4.1288815\n",
      "4.1485777\n",
      "2.0086663\n",
      "5.2877383\n",
      "2.4301996\n",
      "1.4469086\n",
      "2.492595\n",
      "4.6330223\n",
      "2.9046626\n",
      "2.008181\n",
      "4.438901\n",
      "3.7587152\n",
      "3.9875145\n",
      "3.6454256\n",
      "3.5756657\n",
      "2.4968386\n",
      "5.1559124\n",
      "2.342936\n",
      "1.3896391\n",
      "4.0339203\n",
      "4.827164\n",
      "2.2642765\n",
      "2.78342\n",
      "2.940164\n",
      "2.1289766\n",
      "1.5192622\n",
      "0.4766545\n",
      "5.5667777\n",
      "2.2840543\n",
      "4.12164\n",
      "2.3560283\n",
      "2.483855\n",
      "0.22462597\n",
      "7.008109\n",
      "4.0917816\n",
      "2.8996365\n",
      "0.9118706\n",
      "3.447381\n",
      "3.260218\n",
      "5.4944944\n",
      "1.7271608\n",
      "2.3479173\n",
      "2.611058\n",
      "2.5513453\n",
      "1.7777054\n",
      "1.8069578\n",
      "4.3163176\n",
      "3.3534203\n",
      "3.1738782\n",
      "0.65196323\n",
      "2.2216806\n",
      "4.2569857\n",
      "0.947961\n",
      "2.924815\n",
      "1.9083848\n",
      "2.839083\n",
      "3.6370413\n",
      "4.0461526\n",
      "2.1923113\n",
      "5.887887\n",
      "5.38719\n",
      "2.364425\n",
      "0.33369273\n",
      "0.94128954\n",
      "2.0647328\n",
      "2.5131216\n",
      "3.3648214\n",
      "4.213414\n",
      "2.9886\n",
      "3.306713\n",
      "7.0953474\n",
      "3.8635597\n",
      "4.928956\n",
      "0.03650367\n",
      "1.6854028\n",
      "4.3546023\n",
      "1.6780009\n",
      "3.3054335\n",
      "4.350583\n",
      "4.8002186\n",
      "3.4110966\n",
      "1.1177435\n",
      "6.238644\n",
      "1.8602754\n",
      "2.2021477\n",
      "3.8806176\n",
      "1.1156007\n",
      "4.4710407\n",
      "4.123316\n",
      "4.1814995\n",
      "1.97845\n",
      "3.043652\n",
      "2.434278\n",
      "4.3446503\n",
      "1.2806613\n",
      "3.54004\n",
      "4.137431\n",
      "4.6436405\n",
      "3.210666\n",
      "4.1829743\n",
      "1.6786323\n",
      "6.732473\n",
      "4.307622\n",
      "2.9633036\n",
      "4.9656553\n",
      "1.5206726\n",
      "2.027029\n",
      "4.669617\n",
      "3.8580456\n",
      "1.7308589\n",
      "6.2945237\n",
      "0.07027911\n",
      "0.99854183\n",
      "4.2465434\n",
      "3.7383952\n",
      "1.9911968\n",
      "1.8458014\n",
      "4.4884863\n",
      "0.9243398\n",
      "4.9072666\n",
      "3.3248212\n",
      "1.7144561\n",
      "2.7302465\n",
      "3.1814272\n",
      "3.70052\n",
      "2.1619184\n",
      "1.7377756\n",
      "5.1772604\n",
      "1.5425532\n",
      "4.237152\n",
      "3.2769985\n",
      "1.9070199\n",
      "4.334942\n",
      "4.161977\n",
      "3.126051\n",
      "4.310643\n",
      "3.5507827\n",
      "0.6325195\n",
      "3.2185323\n",
      "2.2697508\n",
      "4.4281425\n",
      "3.6908352\n",
      "1.9802158\n",
      "0.03660565\n",
      "5.3170843\n",
      "1.9776878\n",
      "3.3539946\n",
      "3.4303951\n",
      "5.8028216\n",
      "3.497168\n",
      "3.4155507\n",
      "1.7509626\n",
      "4.139108\n",
      "2.411507\n",
      "5.1584926\n",
      "4.146318\n",
      "0.8622974\n",
      "1.6172612\n",
      "4.7616873\n",
      "5.7172885\n",
      "2.984784\n",
      "2.567986\n",
      "0.22727957\n",
      "4.356482\n",
      "8.308529\n",
      "3.6630087\n",
      "2.5917\n",
      "2.2412736\n",
      "2.403476\n",
      "4.778859\n",
      "2.666245\n",
      "4.4508543\n",
      "3.4083636\n",
      "1.2171384\n",
      "4.0751257\n",
      "5.5775914\n",
      "4.392685\n",
      "3.9659758\n",
      "3.6008058\n",
      "3.3268883\n",
      "2.898787\n",
      "5.0345945\n",
      "5.993411\n",
      "2.5229335\n",
      "3.7663324\n",
      "3.5813203\n",
      "3.2343626\n",
      "4.762043\n",
      "2.345335\n",
      "1.333954\n",
      "3.627266\n",
      "2.0449922\n",
      "1.1636453\n",
      "2.5120156\n",
      "3.030857\n",
      "2.9498644\n",
      "3.9414783\n",
      "1.674397\n",
      "4.8365674\n",
      "3.3109035\n",
      "3.27742\n",
      "1.5932417\n",
      "1.7400447\n",
      "1.8071458\n",
      "2.6631675\n",
      "3.2038765\n",
      "7.006268\n",
      "3.2223706\n",
      "3.2685928\n",
      "2.5021725\n",
      "3.1130314\n",
      "3.7881417\n",
      "2.9104016\n",
      "3.1785703\n",
      "3.136582\n",
      "2.2684102\n",
      "0.8910008\n",
      "2.8808773\n",
      "5.1648183\n",
      "1.7471465\n",
      "3.150586\n",
      "5.1933136\n",
      "1.3953705\n",
      "4.206107\n",
      "1.9423457\n",
      "3.1919613\n",
      "5.6494718\n",
      "1.732074\n",
      "0.6850965\n",
      "2.3178205\n",
      "1.909543\n",
      "3.2872853\n",
      "2.6190233\n",
      "4.0514\n",
      "0.842028\n",
      "0.9747873\n",
      "1.966426\n",
      "5.5435147\n",
      "1.745457\n",
      "2.232675\n",
      "6.504218\n",
      "6.6818237\n",
      "3.2379923\n",
      "2.5207226\n",
      "3.68887\n",
      "3.6099849\n",
      "3.1287377\n",
      "1.7408373\n",
      "2.9899807\n",
      "5.3895473\n",
      "3.663121\n",
      "1.7653081\n",
      "4.9276967\n",
      "2.4088373\n",
      "2.0082788\n",
      "4.443597\n",
      "1.4378691\n",
      "3.7638931\n",
      "4.6752377\n",
      "3.2812662\n",
      "3.9771116\n",
      "2.6088364\n",
      "4.564495\n",
      "2.6780596\n",
      "4.9605355\n",
      "2.3346639\n",
      "2.4274\n",
      "4.089782\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-187-22c1fe5c9b17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd_ind_trg\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtrg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_lengths\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtrg_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                     transformer.learning_rate : 1e-2}\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "transformer = Transformer(num_wds_input)\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for itr, train_batch in enumerate(train_it):\n",
    "    src_tensor  = train_batch.src[0].data.cpu().numpy().transpose()\n",
    "    src_len = train_batch.src[1].cpu().numpy()\n",
    "    trg_tensor  = train_batch.trg[0].data.cpu().numpy().transpose()\n",
    "    trg_len = train_batch.trg[1].cpu().numpy()\n",
    "    trg_len = np.ceil(np.random.uniform(size=4)*(trg_len-1)).astype(int)\n",
    "    trg_len = [1, 2, 3, 4]\n",
    "#     print(src_tensor.shape, src_len.shape, trg_tensor.shape, trg_len.shape)\n",
    "#     print(src_tensor, src_len, trg_tensor, trg_len)\n",
    "    trn_feed_dict = {transformer.wd_ind_src : src_tensor, transformer.input_lengths : src_len,\n",
    "                    transformer.wd_ind_trg : trg_tensor, transformer.output_lengths : trg_len,\n",
    "                    transformer.learning_rate : 1e-2}\n",
    "    _,loss = sess.run([transformer.optimizer, transformer.loss], trn_feed_dict)\n",
    "    if itr % 100 == 0:\n",
    "        print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(transformer.prediction_mask, trn_feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 20, 13, 14])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self = transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, m = sess.run([self.encoder.attentionLayers[0].s, self.encoder.attentionLayers[0].mask], trn_feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[-1,-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[-1,-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(s * m)[-1, -1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 33)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(self.wd_ind_src, trn_feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1, 33)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(self.encoder.attentionLayers[0].masked_softmax.v_exp_sum, trn_feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 33, 33)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(self.encoder.attentionLayers[0].masked_softmax.output, trn_feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.8806496, 7.2538576, 6.836245 , 6.8354425, 6.8945065, 7.029778 ,\n",
       "        7.261009 , 7.1071267, 7.1146455, 6.8747187, 6.6536813, 7.0499544,\n",
       "        7.261133 , 7.0428286, 7.2384424, 6.873402 , 7.1073694, 7.1073966,\n",
       "        6.6534758, 7.0587964, 6.6961355, 6.5816846, 6.873296 , 7.0611773,\n",
       "        6.934389 , 7.4266753, 6.873235 , 7.107667 , 7.1076937, 7.2384634,\n",
       "        6.8879128, 7.261527 , 6.8138814, 7.107829 , 7.107856 , 7.2384715,\n",
       "        6.873083 , 7.107937 , 7.107964 , 7.238476 , 7.0586863, 7.1080456,\n",
       "        6.871643 , 6.872977 , 7.1081266, 6.7329335, 7.104479 ],\n",
       "       [      nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan,       nan,\n",
       "              nan,       nan,       nan,       nan,       nan]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=self.wd_ind_trg, logits=self.decoder.output_raw) * self.output_mask, trn_feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
