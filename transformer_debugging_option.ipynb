{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-20c462c49c02>:80: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From <ipython-input-1-20c462c49c02>:84: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9030813d714ebebff6619f6bdb31bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 64.42255\n",
      "1000 5.29291\n",
      "2000 8.302757\n",
      "3000 1.4244095\n",
      "4000 14.254912\n",
      "5000 7.001434\n",
      "6000 3.743309\n",
      "7000 8.526296\n",
      "8000 2.4261403\n",
      "9000 2.5368729\n",
      "10000 7.662856\n",
      "11000 4.6585245\n",
      "12000 1.1795835\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "# !pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib torchtext\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "# Standard PyTorch imports\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib.layers import layer_norm\n",
    "import nn_utils\n",
    "\n",
    "# For plots\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "real = 1\n",
    "BATCH_SIZE = 16\n",
    "#!conda install torchtext spacy\n",
    "# !python -m spacy download en\n",
    "# !python -m spacy download de\n",
    "if real:\n",
    "  from torchtext import data\n",
    "  from torchtext import datasets\n",
    "  import tqdm\n",
    "  import re\n",
    "  import spacy\n",
    "\n",
    "  spacy_de = spacy.load('de')\n",
    "  spacy_en = spacy.load('en')\n",
    "\n",
    "  url = re.compile('(<url>.*</url>)')\n",
    "\n",
    "  def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "  def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "  # Testing IWSLT\n",
    "  DE = data.Field(\n",
    "      tokenize=tokenize_de,\n",
    "      init_token='<bos>',\n",
    "      eos_token='<eos>',\n",
    "      include_lengths=True)\n",
    "  EN = data.Field(\n",
    "      tokenize=tokenize_en,\n",
    "      init_token='<bos>',\n",
    "      eos_token='<eos>',\n",
    "      include_lengths=True)\n",
    "\n",
    "  train, val, test = datasets.IWSLT.splits(\n",
    "      exts=('.de', '.en'), fields=(DE, EN))\n",
    "\n",
    "  train_it = data.Iterator(\n",
    "      train,\n",
    "      batch_size=BATCH_SIZE,\n",
    "      sort_within_batch=True,\n",
    "      train=True,\n",
    "      repeat=False,\n",
    "      shuffle=True)\n",
    "  MIN_WORD_FREQ = 10\n",
    "  MAX_NUM_WORDS = 3000\n",
    "  DE.build_vocab(train.src, min_freq=MIN_WORD_FREQ, max_size=MAX_NUM_WORDS)\n",
    "  EN.build_vocab(train.trg, min_freq=MIN_WORD_FREQ, max_size=MAX_NUM_WORDS)\n",
    "\n",
    "  num_wds_input = len(DE.vocab.itos)\n",
    "  num_wds_output = len(EN.vocab.itos)\n",
    "else:\n",
    "  num_wds_input = 1004\n",
    "\n",
    "\n",
    "class masked_softmax:\n",
    "  def __init__(self, v, mask, dim=2):\n",
    "    #bs, query dimension, key dimension\n",
    "    v_mask = v * mask\n",
    "    v_max = tf.reduce_max(v_mask, dim, keep_dims=True)\n",
    "    v_stable = v_mask - v_max\n",
    "\n",
    "    v_exp = tf.exp(v_stable) * mask\n",
    "    v_exp_sum = tf.reduce_sum(v_exp, dim, keep_dims=True)\n",
    "    self.v_mask, self.v_max, self.v_stable, self.v_exp, self.v_exp_sum = \\\n",
    "        v_mask, v_max, v_stable, v_exp, v_exp_sum\n",
    "    self.output = v_exp / (v_exp_sum + 1e-20)\n",
    "\n",
    "\n",
    "class Encoder:\n",
    "  def __init__(self, num_wds, wd_ind, mask, ndims=64, n_layers=6):\n",
    "    self.num_wds = num_wds\n",
    "    self.wd_ind = wd_ind\n",
    "    self.mask = mask\n",
    "    self.length = tf.shape(self.wd_ind)[1]\n",
    "    self.wd_emb = tf.Variable(\n",
    "        tf.random_uniform([self.num_wds, ndims], minval=-1, maxval=1.))\n",
    "    self.wd_vec = tf.nn.embedding_lookup(self.wd_emb, wd_ind)\n",
    "    self.pos = tf.reshape(\n",
    "        tf.range(tf.cast(self.length, tf.float32), dtype=tf.float32),\n",
    "        (1, -1, 1))\n",
    "    self.divider_exponent = tf.reshape(\n",
    "        tf.range(tf.cast(ndims // 2, tf.float32)),\n",
    "        (1, 1, -1)) * 2. / tf.cast(ndims, tf.float32)\n",
    "    self.divider = tf.pow(10000., self.divider_exponent)\n",
    "    self.input_to_sinusoids = self.pos / self.divider\n",
    "    self.pos_sin = tf.sin(self.input_to_sinusoids)\n",
    "    self.pos_cos = tf.cos(self.input_to_sinusoids)\n",
    "    # self.position = tf.reshape(\n",
    "    #     tf.range(tf.cast(self.length, tf.float32), dtype=tf.float32) / 10000,\n",
    "    #     (1, -1, 1))\n",
    "    self.position = tf.concat((self.pos_sin, self.pos_cos), -1)\n",
    "    self.w_tilde = embedding = self.wd_vec + self.position\n",
    "    self.encoding = []\n",
    "    self.attentionLayers = []\n",
    "    for _ in range(n_layers):\n",
    "      attentionLayer = AttentionLayer(embedding, mask)\n",
    "      embedding = attentionLayer.output\n",
    "      self.encoding.append(embedding)\n",
    "      self.attentionLayers.append(attentionLayer)\n",
    "\n",
    "\n",
    "class AttentionLayer:\n",
    "  def __init__(self, X, mask, X_decode=None, decode_mask=None, ff_layer=True):\n",
    "    bs, length, ndim = [v.value for v in X.shape]\n",
    "    self.X = X\n",
    "    if X_decode is None:\n",
    "      self.q, self.k, self.v = [\n",
    "          tf.tanh(tf.layers.dense(X, ndim)) for _ in range(3)\n",
    "      ]\n",
    "      decode_mask = mask\n",
    "    else:\n",
    "      self.q = tf.tanh(tf.layers.dense(X_decode, ndim))\n",
    "      self.k, self.v = [tf.tanh(tf.layers.dense(X, ndim)) for _ in range(2)]\n",
    "    #batch, attention queries, attention keys, embeddings\n",
    "    self.q_expanded = tf.expand_dims(self.q, 2)\n",
    "    self.k_expanded = tf.expand_dims(self.k, 1)\n",
    "    self.v_expanded = tf.expand_dims(self.v, 1)\n",
    "    self.s_raw = tf.reduce_sum(self.q_expanded * self.k_expanded, -1)\n",
    "    self.mask = tf.expand_dims(decode_mask, 2) * tf.expand_dims(mask, 1)\n",
    "    self.masked_softmax = masked_softmax(self.s_raw, self.mask)\n",
    "    self.s = self.masked_softmax.output\n",
    "    self.a = tf.expand_dims(self.s * self.mask, -1) * self.v_expanded\n",
    "    #A is shape bs, query, key, emb\n",
    "    self.a_compressed = tf.reduce_sum(self.a, 2)\n",
    "    if X_decode is None:\n",
    "      self.e = layer_norm(self.a_compressed + X)\n",
    "    else:\n",
    "      self.e = layer_norm(self.a_compressed + X_decode)\n",
    "    if ff_layer:\n",
    "      self.output = layer_norm(tf.layers.dense(self.e, ndim) + self.e)\n",
    "    else:\n",
    "      self.output = self.e\n",
    "\n",
    "\n",
    "class Decoder:\n",
    "  def __init__(self, num_wds, wd_ind, mask, encoder, ndims=20, n_layers=6):\n",
    "    self.num_wds = num_wds\n",
    "    self.wd_ind = wd_ind\n",
    "    self.mask = mask\n",
    "    self.encoder = encoder\n",
    "    self.length = tf.shape(self.wd_ind)[1]\n",
    "    self.wd_emb = tf.Variable(\n",
    "        tf.random_uniform([self.num_wds, ndims], minval=-1, maxval=1.))\n",
    "    self.wd_vec = tf.nn.embedding_lookup(self.wd_emb, wd_ind)\n",
    "    self.pos = tf.reshape(\n",
    "        tf.range(tf.cast(self.length, tf.float32), dtype=tf.float32),\n",
    "        (1, -1, 1))\n",
    "    self.divider_exponent = tf.reshape(\n",
    "        tf.range(tf.cast(ndims // 2, tf.float32)),\n",
    "        (1, 1, -1)) * 2. / tf.cast(ndims, tf.float32)\n",
    "    self.divider = tf.pow(10000., self.divider_exponent)\n",
    "    self.input_to_sinusoids = self.pos / self.divider\n",
    "    self.pos_sin = tf.sin(self.input_to_sinusoids)\n",
    "    self.pos_cos = tf.cos(self.input_to_sinusoids)\n",
    "    # self.position = tf.reshape(\n",
    "    #     tf.range(tf.cast(self.length, tf.float32), dtype=tf.float32) / 10000,\n",
    "    #     (1, -1, 1))\n",
    "    self.position = tf.concat((self.pos_sin, self.pos_cos), -1)\n",
    "    self.w_tilde = embedding = self.wd_vec + self.position\n",
    "    self.decoding = []\n",
    "    self.self_attentions = []\n",
    "    self.encoder_attentions = []\n",
    "    for l_idx in range(n_layers):\n",
    "      attn = AttentionLayer(embedding, mask, ff_layer=False)\n",
    "      self.self_attentions.append(attn)\n",
    "      encode_attn = AttentionLayer(encoder.encoding[l_idx], encoder.mask,\n",
    "                                   attn.output, mask)\n",
    "      self.encoder_attentions.append(encode_attn)\n",
    "      embedding = encode_attn.output\n",
    "\n",
    "    self.output_raw = tf.layers.dense(embedding, num_wds)\n",
    "    #bs, word in sentence of target, embedding\n",
    "\n",
    "    self.outsoftmax = masked_softmax(self.output_raw, tf.expand_dims(mask, -1), dim=2)\n",
    "    self.output = self.outsoftmax.output\n",
    "\n",
    "class Transformer:\n",
    "  def __init__(self, num_wds):\n",
    "    self.num_wds = num_wds\n",
    "    n_layers = 6\n",
    "    ndims = 256\n",
    "    self.learning_rate = tf.placeholder(tf.float32, None)\n",
    "    self.wd_ind_src = wd_ind_src = tf.placeholder(tf.int32, (None, None))\n",
    "    self.wd_ind_trg = wd_ind_trg = tf.placeholder(tf.int32, (None, None))\n",
    "    self.input_lengths = tf.placeholder(tf.int32, [None])\n",
    "    self.output_lengths = tf.placeholder(tf.int32, [None])\n",
    "    self.input_mask = tf.sequence_mask(\n",
    "        self.input_lengths,\n",
    "        maxlen=tf.shape(self.wd_ind_src)[-1],\n",
    "        dtype=tf.float32)\n",
    "    self.output_mask = tf.sequence_mask(\n",
    "        self.output_lengths,\n",
    "        maxlen=tf.shape(self.wd_ind_trg)[-1],\n",
    "        dtype=tf.float32)\n",
    "    self.encoder = Encoder(\n",
    "        num_wds, wd_ind_src, self.input_mask, n_layers=n_layers, ndims=ndims)\n",
    "    self.decoder = Decoder(\n",
    "        num_wds,\n",
    "        wd_ind_trg,\n",
    "        self.output_mask,\n",
    "        self.encoder,\n",
    "        n_layers=n_layers,\n",
    "        ndims=ndims)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "    self.prediction_mask = tf.concat((tf.zeros(\n",
    "        (tf.shape(self.output_mask)[0], 1)), self.output_mask[:, :-1] - self.output_mask[:, 1:]),\n",
    "                                     1)\n",
    "    self.loss = tf.reduce_mean(\n",
    "        tf.square(\n",
    "            tf.reduce_max(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=self.wd_ind_trg, logits=self.decoder.output_raw) *\n",
    "                self.prediction_mask, 1)))\n",
    "    self.optimizer, self.grad_norm_total = nn_utils.apply_clipped_optimizer(\n",
    "        opt, self.loss)\n",
    "\n",
    "\n",
    "# In[71]:\n",
    "\n",
    "transformer = Transformer(num_wds_input)\n",
    "MAX_LEN = 20\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "if real:\n",
    "  for itr, train_batch in enumerate(tqdm.tqdm_notebook(train_it)):\n",
    "    src_tensor = train_batch.src[0].data.cpu().numpy().transpose()\n",
    "    src_len = train_batch.src[1].cpu().numpy()\n",
    "    trg_tensor = train_batch.trg[0].data.cpu().numpy().transpose()\n",
    "    trg_len = train_batch.trg[1].cpu().numpy()\n",
    "    src_tensor, trg_tensor = [t[:, :MAX_LEN] for t in [src_tensor, trg_tensor]]\n",
    "    src_len, trg_len = [np.clip(t, 0, MAX_LEN) for t in [src_len, trg_len]]\n",
    "    trg_len = np.ceil(\n",
    "        np.random.uniform(size=trg_len.shape[0]) * (trg_len - 1)).astype(int)\n",
    "    trn_feed_dict = {\n",
    "        transformer.wd_ind_src: src_tensor,\n",
    "        transformer.input_lengths: src_len,\n",
    "        transformer.wd_ind_trg: trg_tensor,\n",
    "        transformer.output_lengths: trg_len,\n",
    "        transformer.learning_rate: 1e-2 / (np.sqrt(itr + 3))\n",
    "    }\n",
    "    _, loss = sess.run([transformer.optimizer, transformer.loss],\n",
    "                       trn_feed_dict)\n",
    "    if itr % 1000 == 0:\n",
    "      print(itr, loss)\n",
    "    if itr > 20000:\n",
    "      break\n",
    "else:\n",
    "\n",
    "  src_tensor = np.random.randint(low=0, high=num_wds_input, size=(BATCH_SIZE, 81))\n",
    "  src_len = np.random.randint(2, 81, BATCH_SIZE)\n",
    "  trg_tensor = np.random.randint(low=0, high=num_wds_input, size=(BATCH_SIZE, 84))\n",
    "  trg_len = np.random.randint(2, 84, BATCH_SIZE)\n",
    "\n",
    "  fd = {\n",
    "      transformer.wd_ind_src: src_tensor,\n",
    "      transformer.wd_ind_trg: trg_tensor,\n",
    "      transformer.input_lengths: src_len,\n",
    "      transformer.output_lengths: trg_len,\n",
    "      transformer.learning_rate: 1e-2}\n",
    "  sess.run([transformer.optimizer, transformer.loss], fd)\n",
    "# In[75]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch.trg[1].cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11,  1,  5,  1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 0, 1, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ceil(\n",
    "        np.random.uniform(size=trg_len.shape[0]) * (trg_len - 1)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_MAX_DECODE = MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "src_len_decode = src_len[0:1]\n",
    "src_decode = src_tensor[0:1, :src_len_decode[0]]\n",
    "autoregressive = trg_tensor[0:1, 0:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_end(choice, eos_token=None):\n",
    "    return choice.flatten()[0] == eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(autoregressive.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dec_idx in range(10):\n",
    "    pred = sess.run(\n",
    "        transformer.decoder.outsoftmax.output[:,-1,:], {\n",
    "            transformer.wd_ind_src: src_decode,\n",
    "            transformer.input_lengths: src_len_decode,\n",
    "            transformer.wd_ind_trg: autoregressive,\n",
    "            transformer.output_lengths: np.ones(1)*autoregressive.shape[1],\n",
    "        })\n",
    "    choice = pred.argmax(1)\n",
    "    autoregressive = np.concatenate((autoregressive, np.expand_dims(choice, 0)), -1)\n",
    "    if detect_end(choice, None):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00593889, 0.00026155, 0.00024181, ..., 0.00015679, 0.00019054,\n",
       "        0.00029458]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = [EN.vocab.itos[a] for a in autoregressive.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>',\n",
       " 'it',\n",
       " 'actually',\n",
       " 'it',\n",
       " '<unk>',\n",
       " 'it',\n",
       " 'it',\n",
       " 'with',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00042623, 0.00035116, 0.00084691, 0.0001743 , 0.00045299,\n",
       "        0.00058221, 0.00027083, 0.00034129]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[:,12:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
