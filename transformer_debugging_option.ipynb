{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Warning: no model found for 'de'\n",
      "\n",
      "    Only loading the 'de' tokenizer.\n",
      "\n",
      "\n",
      "    Warning: no model found for 'en'\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-1-69145c1abb06>:84: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From <ipython-input-1-69145c1abb06>:88: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d516ff91fa04c4e9cbc13a08c075733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12306), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loss_mean 51.476025 loss_std 12.626099 loss_min 26.642902 loss_max 85.44374\n",
      "200 loss_mean 37.293224 loss_std 8.756538 loss_min 21.133072 loss_max 62.912003\n",
      "300 loss_mean 37.59685 loss_std 9.864247 loss_min 17.859806 loss_max 62.316414\n",
      "400 loss_mean 33.84445 loss_std 8.606287 loss_min 16.358826 loss_max 59.507896\n",
      "500 loss_mean 34.02646 loss_std 9.37392 loss_min 10.199059 loss_max 59.48942\n",
      "600 loss_mean 32.443905 loss_std 9.06887 loss_min 12.982786 loss_max 54.585754\n",
      "700 loss_mean 31.449831 loss_std 8.041597 loss_min 11.362991 loss_max 52.382874\n",
      "800 loss_mean 31.390495 loss_std 8.292342 loss_min 12.162487 loss_max 52.828674\n",
      "900 loss_mean 28.854658 loss_std 8.7589445 loss_min 9.765327 loss_max 53.9932\n",
      "1000 loss_mean 28.692131 loss_std 8.167957 loss_min 10.482076 loss_max 54.172657\n",
      "['<bos>', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "1100 loss_mean 29.801691 loss_std 9.740038 loss_min 6.784786 loss_max 56.10309\n",
      "1200 loss_mean 28.420649 loss_std 8.766122 loss_min 11.031422 loss_max 53.151867\n",
      "1300 loss_mean 27.833662 loss_std 9.423036 loss_min 8.665308 loss_max 56.01594\n",
      "1400 loss_mean 26.741692 loss_std 8.383988 loss_min 9.583044 loss_max 52.674885\n",
      "1500 loss_mean 26.642237 loss_std 7.9816456 loss_min 12.276228 loss_max 50.123478\n",
      "1600 loss_mean 27.641882 loss_std 8.726474 loss_min 10.006968 loss_max 45.684242\n",
      "1700 loss_mean 25.671144 loss_std 8.323508 loss_min 5.764886 loss_max 47.79196\n",
      "1800 loss_mean 24.410097 loss_std 7.8658504 loss_min 7.069001 loss_max 57.482536\n",
      "1900 loss_mean 24.786028 loss_std 8.803813 loss_min 8.185334 loss_max 53.030666\n",
      "2000 loss_mean 24.892668 loss_std 8.600922 loss_min 6.249029 loss_max 53.089996\n",
      "['<bos>', 'translated', 'reaches', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "2100 loss_mean 22.620476 loss_std 8.295496 loss_min 3.4316077 loss_max 41.989594\n",
      "2200 loss_mean 23.4192 loss_std 7.988375 loss_min 6.031729 loss_max 48.74456\n",
      "2300 loss_mean 22.738142 loss_std 7.966045 loss_min 6.7138596 loss_max 52.925793\n",
      "2400 loss_mean 22.176332 loss_std 7.435802 loss_min 6.361853 loss_max 51.10028\n",
      "2500 loss_mean 23.095003 loss_std 8.196697 loss_min 1.8536222 loss_max 47.87845\n",
      "2600 loss_mean 22.427004 loss_std 8.170251 loss_min 6.9095135 loss_max 50.0197\n",
      "2700 loss_mean 19.772865 loss_std 7.392805 loss_min 5.025711 loss_max 36.767975\n",
      "2800 loss_mean 22.513618 loss_std 8.590757 loss_min 0.59110236 loss_max 47.653137\n",
      "2900 loss_mean 21.328968 loss_std 8.08636 loss_min 2.6757529 loss_max 47.766396\n",
      "3000 loss_mean 21.430235 loss_std 7.399009 loss_min 3.2838185 loss_max 36.90245\n",
      "['<bos>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "3100 loss_mean 21.332548 loss_std 8.10691 loss_min 6.3582497 loss_max 52.903633\n",
      "3200 loss_mean 20.26895 loss_std 6.6714215 loss_min 5.784053 loss_max 38.354404\n",
      "3300 loss_mean 19.934992 loss_std 8.17698 loss_min 4.672167 loss_max 42.82494\n",
      "3400 loss_mean 19.274609 loss_std 7.870159 loss_min 4.6499085 loss_max 52.420353\n",
      "3500 loss_mean 19.112812 loss_std 7.5619874 loss_min 4.0507913 loss_max 37.5505\n",
      "3600 loss_mean 19.386436 loss_std 7.956932 loss_min 2.5384586 loss_max 39.502766\n",
      "3700 loss_mean 19.494724 loss_std 7.7047663 loss_min 0.5667493 loss_max 36.73091\n",
      "3800 loss_mean 18.010399 loss_std 6.9017525 loss_min 1.4990801 loss_max 32.726944\n",
      "3900 loss_mean 17.64873 loss_std 8.002763 loss_min 3.3014772 loss_max 40.69136\n",
      "4000 loss_mean 18.616201 loss_std 8.281812 loss_min 2.0664196 loss_max 39.56031\n",
      "['<bos>', 'worn', '<unk>', '<unk>', '<unk>', ',', ',', ',', ',', ',', ',']\n",
      "4100 loss_mean 19.08356 loss_std 7.3851867 loss_min 4.3305326 loss_max 51.46082\n",
      "4200 loss_mean 19.023396 loss_std 7.9097357 loss_min 5.3238053 loss_max 37.972054\n",
      "4300 loss_mean 18.39017 loss_std 7.00322 loss_min 1.7961025 loss_max 33.24418\n",
      "4400 loss_mean 17.614862 loss_std 7.5983734 loss_min 0.65360045 loss_max 36.733345\n",
      "4500 loss_mean 17.002096 loss_std 7.379165 loss_min 4.386609 loss_max 41.654457\n",
      "4600 loss_mean 17.220928 loss_std 8.755285 loss_min 0.40155232 loss_max 48.095093\n",
      "4700 loss_mean 18.601189 loss_std 7.957177 loss_min 0.4932147 loss_max 36.10377\n",
      "4800 loss_mean 19.352137 loss_std 7.5882273 loss_min 1.9885912 loss_max 39.330193\n",
      "4900 loss_mean 18.477533 loss_std 6.657823 loss_min 2.624917 loss_max 34.98246\n",
      "5000 loss_mean 17.650873 loss_std 7.93421 loss_min 1.3571994 loss_max 41.465004\n",
      "['<bos>', ',', ',', ',', ',', ',', 'to', 'to', 'to', 'to', 'to']\n",
      "5100 loss_mean 17.924213 loss_std 6.9465733 loss_min 5.2919445 loss_max 34.816967\n",
      "5200 loss_mean 16.692526 loss_std 7.6584744 loss_min 3.5325985 loss_max 38.693806\n",
      "5300 loss_mean 18.355158 loss_std 7.833776 loss_min 3.459149 loss_max 39.716843\n",
      "5400 loss_mean 18.197058 loss_std 7.161877 loss_min 1.664627 loss_max 32.693214\n",
      "5500 loss_mean 18.082909 loss_std 7.8769994 loss_min 2.2566273 loss_max 46.803574\n",
      "5600 loss_mean 17.498905 loss_std 7.2481875 loss_min 4.0199633 loss_max 35.77451\n",
      "5700 loss_mean 16.827417 loss_std 7.3124375 loss_min 1.7746787 loss_max 36.866043\n",
      "5800 loss_mean 16.956314 loss_std 7.71163 loss_min 0.9560276 loss_max 38.960655\n",
      "5900 loss_mean 16.688807 loss_std 7.5459065 loss_min 0.75322104 loss_max 32.915016\n",
      "6000 loss_mean 16.903713 loss_std 7.4355454 loss_min 2.9761994 loss_max 40.804947\n",
      "['<bos>', '<unk>', '<unk>', '<unk>', ',', ',', ',', 'is', 'is', 'is', 'is']\n",
      "6100 loss_mean 15.048845 loss_std 6.2411027 loss_min 2.8951807 loss_max 32.643097\n",
      "6200 loss_mean 17.28281 loss_std 8.084286 loss_min 0.21448183 loss_max 44.569855\n",
      "6300 loss_mean 16.923306 loss_std 6.854435 loss_min 4.4139867 loss_max 36.647865\n",
      "6400 loss_mean 16.045235 loss_std 7.0165334 loss_min 0.18058169 loss_max 34.554176\n",
      "6500 loss_mean 17.279263 loss_std 7.4948378 loss_min 0.4081069 loss_max 36.71772\n",
      "6600 loss_mean 16.872408 loss_std 6.6234646 loss_min 3.4981518 loss_max 36.834343\n",
      "6700 loss_mean 15.874752 loss_std 6.8123174 loss_min 0.91685593 loss_max 35.536064\n",
      "6800 loss_mean 16.5362 loss_std 7.244668 loss_min 4.5645223 loss_max 41.202557\n",
      "6900 loss_mean 14.377417 loss_std 7.067729 loss_min 1.4075549 loss_max 35.15754\n",
      "7000 loss_mean 15.855193 loss_std 7.2757306 loss_min 1.9382403 loss_max 39.582832\n",
      "['<bos>', ',', '<unk>', '<unk>', 'to', '<unk>', 'to', '<eos>', '<unk>', 'to', '<unk>']\n",
      "7100 loss_mean 15.289395 loss_std 7.5041656 loss_min 0.23453969 loss_max 33.85873\n",
      "7200 loss_mean 16.29418 loss_std 7.043339 loss_min 0.77409756 loss_max 33.481483\n",
      "7300 loss_mean 15.716841 loss_std 7.1262407 loss_min 0.11201188 loss_max 33.54231\n",
      "7400 loss_mean 15.60761 loss_std 8.779185 loss_min 0.2486146 loss_max 43.478825\n",
      "7500 loss_mean 14.507835 loss_std 7.296683 loss_min 0.7285332 loss_max 36.451233\n",
      "7600 loss_mean 16.792269 loss_std 7.2189713 loss_min 4.2463603 loss_max 40.32972\n",
      "7700 loss_mean 15.414546 loss_std 7.3383765 loss_min 0.2157404 loss_max 42.6845\n",
      "7800 loss_mean 15.247615 loss_std 6.4865923 loss_min 2.8095777 loss_max 36.761566\n",
      "7900 loss_mean 15.921979 loss_std 7.0825024 loss_min 0.056383442 loss_max 36.470573\n",
      "8000 loss_mean 15.810829 loss_std 6.024267 loss_min 2.4992664 loss_max 27.900337\n",
      "['<bos>', 'you', '<unk>', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to']\n",
      "8100 loss_mean 17.404987 loss_std 7.4994287 loss_min 2.5443068 loss_max 35.480965\n",
      "8200 loss_mean 15.200617 loss_std 7.1526895 loss_min 2.7006097 loss_max 41.983063\n",
      "8300 loss_mean 15.332499 loss_std 8.048409 loss_min 1.2736251 loss_max 41.833332\n",
      "8400 loss_mean 15.043178 loss_std 7.1680517 loss_min 0.42142797 loss_max 40.847233\n",
      "8500 loss_mean 15.266981 loss_std 7.347972 loss_min 0.37981084 loss_max 36.89707\n",
      "8600 loss_mean 15.353869 loss_std 7.1457505 loss_min 0.9751599 loss_max 37.497406\n",
      "8700 loss_mean 14.708074 loss_std 6.4551325 loss_min 2.7837372 loss_max 33.364105\n",
      "8800 loss_mean 15.276051 loss_std 6.502726 loss_min 0.33592904 loss_max 30.090816\n",
      "8900 loss_mean 14.954991 loss_std 7.5762825 loss_min 0.2497467 loss_max 43.56179\n",
      "9000 loss_mean 14.231837 loss_std 7.7132797 loss_min 0.196911 loss_max 35.58681\n",
      "['<bos>', ',', ',', ',', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "9100 loss_mean 14.412151 loss_std 7.4406066 loss_min 0.24503583 loss_max 37.064434\n",
      "9200 loss_mean 14.300804 loss_std 7.471674 loss_min 0.9134231 loss_max 39.51338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9300 loss_mean 14.700266 loss_std 6.527357 loss_min 0.46171954 loss_max 30.16943\n",
      "9400 loss_mean 13.518267 loss_std 6.994354 loss_min 0.9813534 loss_max 31.448917\n",
      "9500 loss_mean 14.548695 loss_std 7.0417233 loss_min 3.2677855 loss_max 36.77241\n",
      "9600 loss_mean 14.738674 loss_std 5.9524465 loss_min 2.7007308 loss_max 31.728958\n",
      "9700 loss_mean 14.599072 loss_std 7.622949 loss_min 1.5258963 loss_max 34.438484\n",
      "9800 loss_mean 15.007189 loss_std 7.5095916 loss_min 1.3961005 loss_max 37.79969\n",
      "9900 loss_mean 14.841008 loss_std 8.603515 loss_min 0.8038865 loss_max 38.003654\n",
      "10000 loss_mean 14.834167 loss_std 7.307744 loss_min 0.60740757 loss_max 34.54245\n",
      "['<bos>', ',', 'to', 'to', 'that', 'that', 'that', 'dangers', 'is', 'is', 'is']\n",
      "10100 loss_mean 14.249538 loss_std 7.151873 loss_min 0.61586386 loss_max 31.221218\n",
      "10200 loss_mean 14.523155 loss_std 6.9445753 loss_min 0.23923245 loss_max 36.593807\n",
      "10300 loss_mean 16.047361 loss_std 7.524457 loss_min 0.15171422 loss_max 35.19431\n",
      "10400 loss_mean 14.516238 loss_std 6.663596 loss_min 2.8042767 loss_max 33.835175\n",
      "10500 loss_mean 14.354964 loss_std 6.593254 loss_min 1.0193022 loss_max 29.29646\n",
      "10600 loss_mean 15.588824 loss_std 6.865286 loss_min 3.584688 loss_max 35.89523\n",
      "10700 loss_mean 14.935951 loss_std 6.77755 loss_min 0.07933478 loss_max 34.525513\n",
      "10800 loss_mean 14.183473 loss_std 6.709192 loss_min 0.8168256 loss_max 33.687363\n",
      "10900 loss_mean 13.650359 loss_std 6.6763844 loss_min 0.78143144 loss_max 32.79694\n",
      "11000 loss_mean 13.337347 loss_std 6.8295183 loss_min 0.37875402 loss_max 36.17953\n",
      "['<bos>', 'to', '<unk>', '<eos>', '<eos>', '<unk>', '<eos>', '<eos>', '<eos>', '<eos>', '<unk>']\n",
      "11100 loss_mean 15.254708 loss_std 6.6307974 loss_min 1.541583 loss_max 30.352257\n",
      "11200 loss_mean 15.953721 loss_std 8.189567 loss_min 1.0707803 loss_max 40.775208\n",
      "11300 loss_mean 15.843047 loss_std 7.2814465 loss_min 1.494303 loss_max 37.649433\n",
      "11400 loss_mean 13.792683 loss_std 6.743501 loss_min 0.0987583 loss_max 30.004503\n",
      "11500 loss_mean 14.531061 loss_std 6.287903 loss_min 0.31609935 loss_max 26.737886\n",
      "11600 loss_mean 14.067195 loss_std 7.1262503 loss_min 2.1968114 loss_max 36.6776\n",
      "11700 loss_mean 14.070272 loss_std 5.889728 loss_min 3.1892283 loss_max 30.99044\n",
      "11800 loss_mean 15.761301 loss_std 6.9223385 loss_min 2.264505 loss_max 35.38217\n",
      "11900 loss_mean 14.323523 loss_std 6.195371 loss_min 0.8073739 loss_max 32.609364\n",
      "12000 loss_mean 13.795176 loss_std 7.010025 loss_min 0.20287597 loss_max 31.012676\n",
      "['<bos>', ',', 'in', '<eos>', 'of', 'of', 'We', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "12100 loss_mean 14.964778 loss_std 7.706933 loss_min 0.31204414 loss_max 45.06267\n",
      "12200 loss_mean 13.543357 loss_std 7.594806 loss_min 2.2299323 loss_max 37.183952\n",
      "12300 loss_mean 14.685537 loss_std 7.268148 loss_min 1.294677 loss_max 32.94949\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e44bb329d2a4c6eb77aaf3070ae8059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12306), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12400 loss_mean 14.736526 loss_std 6.814895 loss_min 0.053014867 loss_max 32.84482\n",
      "12500 loss_mean 14.506157 loss_std 6.6233845 loss_min 3.6254475 loss_max 33.253746\n",
      "12600 loss_mean 14.34903 loss_std 6.174605 loss_min 0.5172762 loss_max 30.965567\n",
      "12700 loss_mean 14.865951 loss_std 6.469075 loss_min 1.4860168 loss_max 33.172398\n",
      "12800 loss_mean 14.609281 loss_std 7.657356 loss_min 0.23216556 loss_max 36.552063\n",
      "12900 loss_mean 14.743555 loss_std 5.9788723 loss_min 1.4544481 loss_max 35.74065\n",
      "13000 loss_mean 14.593563 loss_std 6.0436907 loss_min 3.478807 loss_max 36.51232\n",
      "['<bos>', '.', '.', '.', ',', '<unk>', 'more', 'to', '<unk>', '<unk>', 'kind']\n",
      "13100 loss_mean 13.130924 loss_std 7.464578 loss_min 0.08939794 loss_max 35.02022\n",
      "13200 loss_mean 13.993592 loss_std 6.5846157 loss_min 1.6950214 loss_max 31.72578\n",
      "13300 loss_mean 12.759207 loss_std 6.151493 loss_min 2.0911474 loss_max 30.15445\n",
      "13400 loss_mean 14.020524 loss_std 6.6179433 loss_min 2.6937296 loss_max 32.607193\n",
      "13500 loss_mean 13.637503 loss_std 6.5289335 loss_min 0.23914826 loss_max 43.834686\n",
      "13600 loss_mean 14.316449 loss_std 7.323423 loss_min 0.4985615 loss_max 37.83208\n",
      "13700 loss_mean 13.368153 loss_std 6.432667 loss_min 0.8925882 loss_max 30.77946\n",
      "13800 loss_mean 13.323095 loss_std 6.9072747 loss_min 0.3182373 loss_max 34.433926\n",
      "13900 loss_mean 13.7192745 loss_std 6.5999703 loss_min 0.36125636 loss_max 34.239346\n",
      "14000 loss_mean 13.943712 loss_std 6.2661314 loss_min 1.7618909 loss_max 32.251614\n",
      "['<bos>', '.', '.', 'with', '<eos>', '<eos>', 'to', 'over', '.', '.', 'are']\n",
      "14100 loss_mean 14.1553135 loss_std 6.7412066 loss_min 0.10406253 loss_max 27.494946\n",
      "14200 loss_mean 13.246869 loss_std 6.493114 loss_min 0.30875003 loss_max 28.748207\n",
      "14300 loss_mean 14.289626 loss_std 7.065351 loss_min 0.6720085 loss_max 30.999\n",
      "14400 loss_mean 13.951805 loss_std 7.1916714 loss_min 0.48448816 loss_max 34.037056\n",
      "14500 loss_mean 14.197873 loss_std 7.440187 loss_min 0.015407787 loss_max 32.092365\n",
      "14600 loss_mean 14.186006 loss_std 6.3071923 loss_min 0.76425266 loss_max 34.52532\n",
      "14700 loss_mean 13.701299 loss_std 7.2549744 loss_min 2.015399 loss_max 35.05276\n",
      "14800 loss_mean 13.248445 loss_std 6.5165896 loss_min 0.5013132 loss_max 33.784855\n",
      "14900 loss_mean 14.258474 loss_std 6.9898252 loss_min 1.157114 loss_max 28.247925\n",
      "15000 loss_mean 13.23168 loss_std 7.4857445 loss_min 0.03200364 loss_max 29.61165\n",
      "['<bos>', ',', 'a', 'a', 'a', 'time', 'the', '<unk>', 'that', 'over', \"'s\"]\n",
      "15100 loss_mean 12.994851 loss_std 6.1605806 loss_min 0.18452118 loss_max 28.790041\n",
      "15200 loss_mean 13.138203 loss_std 6.2895603 loss_min 0.83512336 loss_max 27.596947\n",
      "15300 loss_mean 13.4932785 loss_std 6.6383896 loss_min 1.0095084 loss_max 34.992867\n",
      "15400 loss_mean 12.669969 loss_std 7.2879295 loss_min 0.31164664 loss_max 39.19903\n",
      "15500 loss_mean 14.87193 loss_std 7.0061793 loss_min 0.7484477 loss_max 34.935585\n",
      "15600 loss_mean 14.216431 loss_std 6.462317 loss_min 2.3777747 loss_max 34.14851\n",
      "15700 loss_mean 14.032635 loss_std 6.4760013 loss_min 1.2268307 loss_max 36.800747\n",
      "15800 loss_mean 12.983997 loss_std 6.3992896 loss_min 1.309582 loss_max 28.843958\n",
      "15900 loss_mean 14.613167 loss_std 7.5384874 loss_min 0.97681946 loss_max 33.10412\n",
      "16000 loss_mean 13.429246 loss_std 7.0965233 loss_min 0.061300777 loss_max 32.225197\n",
      "['<bos>', ',', 'a', '.', '<unk>', '<unk>', 'thing', '.', 'I', 'We', 'is']\n",
      "16100 loss_mean 13.254472 loss_std 7.114339 loss_min 0.11014575 loss_max 37.50857\n",
      "16200 loss_mean 13.798457 loss_std 6.6551347 loss_min 0.11416493 loss_max 28.620937\n",
      "16300 loss_mean 14.932272 loss_std 7.6112247 loss_min 0.39228535 loss_max 38.26483\n",
      "16400 loss_mean 13.764739 loss_std 7.2109585 loss_min 0.08070259 loss_max 34.287674\n",
      "16500 loss_mean 13.395425 loss_std 6.997066 loss_min 0.9096573 loss_max 32.039726\n",
      "16600 loss_mean 13.063103 loss_std 6.320643 loss_min 0.8266774 loss_max 26.810541\n",
      "16700 loss_mean 14.23925 loss_std 6.212552 loss_min 2.0977204 loss_max 29.578426\n",
      "16800 loss_mean 11.787776 loss_std 6.7060366 loss_min 1.1561478 loss_max 29.631193\n",
      "16900 loss_mean 12.682605 loss_std 6.9898205 loss_min 0.100250214 loss_max 34.19973\n",
      "17000 loss_mean 14.365809 loss_std 6.883206 loss_min 1.7342132 loss_max 32.225784\n",
      "['<bos>', ',', '<unk>', ',', '<unk>', ',', '<unk>', 'of', 'to', '<unk>', '<unk>']\n",
      "17100 loss_mean 12.676584 loss_std 6.6739182 loss_min 0.17927952 loss_max 30.87934\n",
      "17200 loss_mean 12.155923 loss_std 5.9569087 loss_min 0.10890558 loss_max 30.568052\n",
      "17300 loss_mean 13.909131 loss_std 6.9945455 loss_min 0.2094065 loss_max 39.15368\n",
      "17400 loss_mean 13.756536 loss_std 6.1173716 loss_min 0.031242112 loss_max 34.15322\n",
      "17500 loss_mean 12.63048 loss_std 6.839484 loss_min 0.22830406 loss_max 36.495197\n",
      "17600 loss_mean 12.926111 loss_std 5.8642945 loss_min 1.3283676 loss_max 27.857357\n",
      "17700 loss_mean 12.527223 loss_std 5.9304585 loss_min 0.22363515 loss_max 26.49047\n",
      "17800 loss_mean 14.123436 loss_std 6.3001704 loss_min 0.7370357 loss_max 28.349596\n",
      "17900 loss_mean 13.505441 loss_std 7.386914 loss_min 0.07363201 loss_max 36.364983\n",
      "18000 loss_mean 13.302372 loss_std 7.0358195 loss_min 0.43357617 loss_max 40.88022\n",
      "['<bos>', ',', 'is', 'you', 'as', 'to', 'it', 'I', 'that', 'of', '.']\n",
      "18100 loss_mean 13.208949 loss_std 6.5319653 loss_min 0.016054448 loss_max 32.025826\n",
      "18200 loss_mean 14.950418 loss_std 7.13018 loss_min 0.43999302 loss_max 32.531357\n",
      "18300 loss_mean 12.900831 loss_std 6.4578614 loss_min 0.7271424 loss_max 29.519293\n",
      "18400 loss_mean 13.413689 loss_std 6.86438 loss_min 0.11752488 loss_max 37.230312\n",
      "18500 loss_mean 13.228045 loss_std 6.4224205 loss_min 0.35621482 loss_max 31.362001\n",
      "18600 loss_mean 14.143616 loss_std 6.234583 loss_min 3.2971334 loss_max 33.62607\n",
      "18700 loss_mean 13.078155 loss_std 6.2426124 loss_min 1.2710973 loss_max 32.05981\n",
      "18800 loss_mean 15.190943 loss_std 6.405872 loss_min 1.5545883 loss_max 32.225002\n",
      "18900 loss_mean 12.492018 loss_std 6.450216 loss_min 0.3017135 loss_max 31.22262\n",
      "19000 loss_mean 12.794848 loss_std 7.0335536 loss_min 0.056253143 loss_max 31.968168\n",
      "['<bos>', ',', '.', '?', '.', 'think', '.', 'in', 'was', 'in', 'I']\n",
      "19100 loss_mean 14.047015 loss_std 7.288597 loss_min 0.7432076 loss_max 36.638336\n",
      "19200 loss_mean 13.174283 loss_std 6.487464 loss_min 0.2213691 loss_max 31.212627\n",
      "19300 loss_mean 12.5602255 loss_std 7.4522457 loss_min 0.0130712595 loss_max 32.689636\n",
      "19400 loss_mean 13.548497 loss_std 7.0697126 loss_min 0.3746779 loss_max 35.41579\n",
      "19500 loss_mean 13.265984 loss_std 6.5645747 loss_min 0.16671228 loss_max 29.002728\n",
      "19600 loss_mean 12.484383 loss_std 6.8619404 loss_min 0.08078337 loss_max 31.469734\n",
      "19700 loss_mean 13.562782 loss_std 6.0405655 loss_min 0.014057604 loss_max 38.148643\n",
      "19800 loss_mean 13.951956 loss_std 6.5420523 loss_min 0.028592974 loss_max 30.269325\n",
      "19900 loss_mean 13.171298 loss_std 6.555499 loss_min 0.1313312 loss_max 28.918335\n",
      "20000 loss_mean 14.248066 loss_std 6.895715 loss_min 2.0202322 loss_max 35.399696\n",
      "['<bos>', ',', '.', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "20100 loss_mean 12.765198 loss_std 6.5058446 loss_min 0.030250285 loss_max 33.217716\n",
      "20200 loss_mean 13.000489 loss_std 6.304847 loss_min 2.0128849 loss_max 33.08544\n",
      "20300 loss_mean 14.019171 loss_std 7.484604 loss_min 1.5318817 loss_max 36.019756\n",
      "20400 loss_mean 12.8596945 loss_std 6.4573255 loss_min 1.6325982 loss_max 31.615755\n",
      "20500 loss_mean 12.658621 loss_std 6.68814 loss_min 0.13055901 loss_max 29.9322\n",
      "20600 loss_mean 14.118294 loss_std 6.583483 loss_min 0.96299034 loss_max 28.924986\n",
      "20700 loss_mean 12.785083 loss_std 5.929365 loss_min 0.23101366 loss_max 28.768867\n",
      "20800 loss_mean 13.108551 loss_std 7.537308 loss_min 0.48016217 loss_max 31.773706\n",
      "20900 loss_mean 13.223575 loss_std 5.849553 loss_min 0.06527391 loss_max 27.764599\n",
      "21000 loss_mean 13.125849 loss_std 6.0822325 loss_min 0.17611262 loss_max 28.475006\n",
      "['<bos>', 'Why', 'all', 'it', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "21100 loss_mean 14.553684 loss_std 6.6019397 loss_min 1.358407 loss_max 32.754044\n",
      "21200 loss_mean 13.030124 loss_std 6.607679 loss_min 0.2323234 loss_max 30.582426\n",
      "21300 loss_mean 13.975502 loss_std 6.115634 loss_min 0.86065376 loss_max 37.171783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21400 loss_mean 12.852788 loss_std 6.1127768 loss_min 0.08050847 loss_max 30.489552\n",
      "21500 loss_mean 13.619551 loss_std 6.8504324 loss_min 1.2747835 loss_max 33.144196\n",
      "21600 loss_mean 13.172037 loss_std 6.159949 loss_min 0.5850219 loss_max 29.750801\n",
      "21700 loss_mean 12.661195 loss_std 6.2308564 loss_min 0.21816522 loss_max 31.570555\n",
      "21800 loss_mean 12.467774 loss_std 6.614501 loss_min 0.4162619 loss_max 31.930447\n",
      "21900 loss_mean 13.179471 loss_std 6.5771346 loss_min 0.14309283 loss_max 41.341274\n",
      "22000 loss_mean 12.436319 loss_std 5.4508567 loss_min 0.2870277 loss_max 26.949848\n",
      "['<bos>', 'a', 'to', 'to', 'in', 'in', 'in', '.', 'That', ',', '<unk>']\n",
      "22100 loss_mean 13.50678 loss_std 6.5044003 loss_min 0.2546462 loss_max 29.000555\n",
      "22200 loss_mean 14.185298 loss_std 6.655943 loss_min 0.79388607 loss_max 34.51637\n",
      "22300 loss_mean 12.637961 loss_std 6.911795 loss_min 0.23218146 loss_max 34.763626\n",
      "22400 loss_mean 13.319734 loss_std 5.87173 loss_min 0.81664413 loss_max 27.775196\n",
      "22500 loss_mean 13.511582 loss_std 6.2965293 loss_min 2.55061 loss_max 29.257254\n",
      "22600 loss_mean 12.537079 loss_std 6.575744 loss_min 0.3952362 loss_max 33.33281\n",
      "22700 loss_mean 13.743455 loss_std 6.8706346 loss_min 1.244826 loss_max 35.832207\n",
      "22800 loss_mean 13.586549 loss_std 6.655171 loss_min 0.74283844 loss_max 29.39303\n",
      "22900 loss_mean 13.120578 loss_std 5.725766 loss_min 1.8994262 loss_max 27.690527\n",
      "23000 loss_mean 12.317318 loss_std 6.6133437 loss_min 0.27767184 loss_max 29.57674\n",
      "['<bos>', 'Because', 'a', '.', 'are', '<unk>', \"n't\", 'are', '<unk>', 'We', '<unk>']\n",
      "23100 loss_mean 13.1330185 loss_std 7.96038 loss_min 0.02147633 loss_max 46.87901\n",
      "23200 loss_mean 12.710182 loss_std 6.5895333 loss_min 0.9986778 loss_max 30.125181\n",
      "23300 loss_mean 12.494155 loss_std 5.5944734 loss_min 2.0499556 loss_max 29.828957\n",
      "23400 loss_mean 11.181126 loss_std 6.5366616 loss_min 0.11340814 loss_max 32.31062\n",
      "23500 loss_mean 12.526354 loss_std 6.4709067 loss_min 0.31910345 loss_max 28.825905\n",
      "23600 loss_mean 12.637014 loss_std 6.0933022 loss_min 0.044821788 loss_max 26.784664\n",
      "23700 loss_mean 13.208532 loss_std 6.864096 loss_min 0.44632685 loss_max 31.825926\n",
      "23800 loss_mean 12.23002 loss_std 6.868356 loss_min 0.19268669 loss_max 36.173225\n",
      "23900 loss_mean 12.99135 loss_std 6.1310067 loss_min 0.7782475 loss_max 29.538107\n",
      "24000 loss_mean 12.731629 loss_std 6.684177 loss_min 0.15795004 loss_max 30.707533\n",
      "['<bos>', 'you', '.', '.', '.', '.', '.', '.', '<unk>', '.', '<unk>']\n",
      "24100 loss_mean 13.936077 loss_std 6.434613 loss_min 1.0144047 loss_max 34.894157\n",
      "24200 loss_mean 13.239513 loss_std 6.634575 loss_min 0.0055318205 loss_max 33.809845\n",
      "24300 loss_mean 13.195808 loss_std 6.9921994 loss_min 1.6175226 loss_max 36.69158\n",
      "24400 loss_mean 12.888137 loss_std 7.467625 loss_min 0.41973776 loss_max 30.425491\n",
      "24500 loss_mean 13.123876 loss_std 6.3091326 loss_min 0.14880557 loss_max 30.611082\n",
      "24600 loss_mean 13.7300625 loss_std 6.4691315 loss_min 0.22379626 loss_max 29.982273\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df07a5aeee744f4f89a57077bd14e2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12306), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24700 loss_mean 13.6941395 loss_std 6.166013 loss_min 0.09461602 loss_max 35.378975\n",
      "24800 loss_mean 13.522131 loss_std 7.2986217 loss_min 0.077760935 loss_max 41.778767\n",
      "24900 loss_mean 12.9293 loss_std 7.5191035 loss_min 0.60505366 loss_max 30.613993\n",
      "25000 loss_mean 13.129961 loss_std 6.5929947 loss_min 0.07855506 loss_max 34.934807\n",
      "['<bos>', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'is', '<unk>']\n",
      "25100 loss_mean 12.51868 loss_std 6.8567204 loss_min 0.24519914 loss_max 37.27026\n",
      "25200 loss_mean 13.589473 loss_std 7.359705 loss_min 1.0915486 loss_max 35.44741\n",
      "25300 loss_mean 12.3281555 loss_std 6.990785 loss_min 0.24308649 loss_max 33.291374\n",
      "25400 loss_mean 12.369857 loss_std 6.27489 loss_min 0.07141957 loss_max 34.587288\n",
      "25500 loss_mean 12.366999 loss_std 7.828096 loss_min 0.025999984 loss_max 38.341312\n",
      "25600 loss_mean 11.997226 loss_std 6.833388 loss_min 0.63391477 loss_max 36.02401\n",
      "25700 loss_mean 12.878513 loss_std 7.9988866 loss_min 0.044315625 loss_max 34.614765\n",
      "25800 loss_mean 13.818664 loss_std 6.4265738 loss_min 1.3581125 loss_max 40.577866\n",
      "25900 loss_mean 13.721996 loss_std 7.580182 loss_min 0.1850659 loss_max 35.04609\n",
      "26000 loss_mean 13.117432 loss_std 6.8299546 loss_min 0.18251938 loss_max 31.268349\n",
      "['<bos>', 'to', 'of', '.', ',', 'that', 'that', 'really', ',', 'I', 'were']\n",
      "26100 loss_mean 12.916952 loss_std 5.709119 loss_min 2.6796663 loss_max 32.833057\n",
      "26200 loss_mean 13.553954 loss_std 6.9011364 loss_min 0.20267066 loss_max 31.28051\n",
      "26300 loss_mean 12.700295 loss_std 6.986051 loss_min 1.1814288 loss_max 35.338696\n",
      "26400 loss_mean 12.093243 loss_std 5.74244 loss_min 0.17204526 loss_max 22.801592\n",
      "26500 loss_mean 12.589596 loss_std 6.9049153 loss_min 0.0034440164 loss_max 33.196625\n",
      "26600 loss_mean 12.639343 loss_std 6.186329 loss_min 0.13134938 loss_max 28.747416\n",
      "26700 loss_mean 14.36512 loss_std 6.825213 loss_min 2.3546748 loss_max 38.197372\n",
      "26800 loss_mean 13.801791 loss_std 7.134398 loss_min 0.07776844 loss_max 33.24318\n",
      "26900 loss_mean 11.674603 loss_std 5.792 loss_min 1.037669 loss_max 28.946428\n",
      "27000 loss_mean 12.478159 loss_std 7.0376883 loss_min 0.57058364 loss_max 36.144836\n",
      "['<bos>', 'to', 'to', 'to', 'to', '<unk>', ',', ',', '<unk>', '<unk>', '<unk>']\n",
      "27100 loss_mean 13.737365 loss_std 7.470968 loss_min 0.1637926 loss_max 36.213356\n",
      "27200 loss_mean 11.921374 loss_std 6.4411664 loss_min 0.53173774 loss_max 30.771694\n",
      "27300 loss_mean 12.900178 loss_std 7.0191083 loss_min 0.12926085 loss_max 32.921814\n",
      "27400 loss_mean 12.214834 loss_std 6.6008263 loss_min 0.5033355 loss_max 31.27788\n",
      "27500 loss_mean 13.319433 loss_std 5.78207 loss_min 1.0426129 loss_max 30.940704\n",
      "27600 loss_mean 11.446225 loss_std 5.9072027 loss_min 0.5272602 loss_max 24.230635\n",
      "27700 loss_mean 12.755267 loss_std 6.0281334 loss_min 0.065112524 loss_max 26.079336\n",
      "27800 loss_mean 12.752328 loss_std 7.2042446 loss_min 0.9630789 loss_max 34.149643\n",
      "27900 loss_mean 11.487544 loss_std 5.73108 loss_min 0.07847262 loss_max 26.697777\n",
      "28000 loss_mean 13.212438 loss_std 7.0351787 loss_min 0.51703167 loss_max 35.366478\n",
      "['<bos>', 'to', 'that', ',', 'the', ',', 'the', ',', 'the', 'the', ',']\n",
      "28100 loss_mean 12.472605 loss_std 6.9310617 loss_min 0.27086022 loss_max 41.17775\n",
      "28200 loss_mean 12.765983 loss_std 5.962617 loss_min 1.8185731 loss_max 28.204765\n",
      "28300 loss_mean 11.469893 loss_std 7.0679255 loss_min 0.26953384 loss_max 37.00434\n",
      "28400 loss_mean 12.436268 loss_std 6.318995 loss_min 0.43129665 loss_max 29.856506\n",
      "28500 loss_mean 12.989102 loss_std 6.2682905 loss_min 0.46201432 loss_max 27.536638\n",
      "28600 loss_mean 14.430061 loss_std 6.741614 loss_min 2.1054926 loss_max 35.22194\n",
      "28700 loss_mean 12.618041 loss_std 6.929574 loss_min 0.4537041 loss_max 29.981045\n",
      "28800 loss_mean 11.113396 loss_std 5.9230146 loss_min 0.46782097 loss_max 26.137558\n",
      "28900 loss_mean 12.720819 loss_std 6.735909 loss_min 0.17646301 loss_max 32.812\n",
      "29000 loss_mean 13.231803 loss_std 7.1654816 loss_min 0.041076597 loss_max 31.04212\n",
      "['<bos>', 'to', 'to', 'to', ',', ',', 'that', 'that', ',', 'the', '<unk>']\n",
      "29100 loss_mean 12.098606 loss_std 5.824156 loss_min 1.1535035 loss_max 29.186152\n",
      "29200 loss_mean 14.416796 loss_std 6.3967476 loss_min 0.31356552 loss_max 30.598248\n",
      "29300 loss_mean 13.295637 loss_std 7.1530046 loss_min 0.18406442 loss_max 32.768303\n",
      "29400 loss_mean 12.101489 loss_std 6.203204 loss_min 0.32599857 loss_max 25.905361\n",
      "29500 loss_mean 13.4337845 loss_std 6.2851076 loss_min 0.07452754 loss_max 31.186403\n",
      "29600 loss_mean 12.162658 loss_std 6.073092 loss_min 0.31284103 loss_max 24.9766\n",
      "29700 loss_mean 12.188321 loss_std 6.2257967 loss_min 1.5823543 loss_max 32.713566\n",
      "29800 loss_mean 12.069843 loss_std 6.821693 loss_min 0.40507063 loss_max 28.12281\n",
      "29900 loss_mean 14.004014 loss_std 7.007565 loss_min 2.694625 loss_max 35.477802\n",
      "30000 loss_mean 13.534687 loss_std 6.5904245 loss_min 0.014274394 loss_max 35.327118\n",
      "['<bos>', 'to', '<unk>', '<unk>', 'of', '<unk>', 'the', 'that', 'one', ',', ',']\n",
      "30100 loss_mean 12.772558 loss_std 6.4458146 loss_min 1.7453586 loss_max 29.262436\n",
      "30200 loss_mean 12.643513 loss_std 6.325138 loss_min 1.73065 loss_max 36.9149\n",
      "30300 loss_mean 13.381442 loss_std 6.5933733 loss_min 0.027133971 loss_max 30.036037\n",
      "30400 loss_mean 11.270585 loss_std 6.816218 loss_min 0.0314053 loss_max 32.760723\n",
      "30500 loss_mean 13.026765 loss_std 6.175952 loss_min 1.5813184 loss_max 26.966204\n",
      "30600 loss_mean 13.570651 loss_std 6.350508 loss_min 2.4577448 loss_max 30.071976\n",
      "30700 loss_mean 12.318365 loss_std 6.5518165 loss_min 1.6945912 loss_max 28.794357\n",
      "30800 loss_mean 13.349645 loss_std 6.6705766 loss_min 0.115291394 loss_max 31.59219\n",
      "30900 loss_mean 12.186878 loss_std 6.328635 loss_min 0.106487475 loss_max 31.580736\n",
      "31000 loss_mean 14.251731 loss_std 7.1877227 loss_min 0.28517726 loss_max 33.66899\n",
      "['<bos>', ',', 'I', 'I', ',', ',', 'that', 'that', ',', 'that', ',']\n",
      "31100 loss_mean 11.592957 loss_std 6.95026 loss_min 0.0889615 loss_max 28.23255\n",
      "31200 loss_mean 12.834617 loss_std 6.81662 loss_min 0.51587903 loss_max 38.451286\n",
      "31300 loss_mean 13.267883 loss_std 6.6083155 loss_min 1.4821736 loss_max 30.980415\n",
      "31400 loss_mean 11.983336 loss_std 5.837287 loss_min 0.016384384 loss_max 29.436779\n",
      "31500 loss_mean 13.475509 loss_std 6.678208 loss_min 2.5772643 loss_max 29.374489\n",
      "31600 loss_mean 13.677798 loss_std 7.3730645 loss_min 0.18165934 loss_max 33.330215\n",
      "31700 loss_mean 12.852909 loss_std 6.022218 loss_min 1.5285225 loss_max 26.794487\n",
      "31800 loss_mean 12.759146 loss_std 6.5535054 loss_min 0.51595503 loss_max 28.575039\n",
      "31900 loss_mean 12.4525585 loss_std 6.497324 loss_min 0.7809587 loss_max 27.928467\n",
      "32000 loss_mean 13.469049 loss_std 6.374025 loss_min 0.21943972 loss_max 33.656742\n",
      "['<bos>', ',', ',', ',', ',', ',', 'that', 'that', ',', ',', 'were']\n",
      "32100 loss_mean 12.739765 loss_std 6.442662 loss_min 0.30740985 loss_max 27.573544\n",
      "32200 loss_mean 12.450958 loss_std 6.0037403 loss_min 0.10844094 loss_max 26.0312\n",
      "32300 loss_mean 13.122427 loss_std 6.917714 loss_min 0.22937405 loss_max 30.40182\n",
      "32400 loss_mean 12.640901 loss_std 6.4407616 loss_min 0.4780466 loss_max 35.004723\n",
      "32500 loss_mean 12.499973 loss_std 6.246667 loss_min 0.27232593 loss_max 27.36353\n",
      "32600 loss_mean 12.803339 loss_std 6.0543437 loss_min 0.64212924 loss_max 30.438889\n",
      "32700 loss_mean 12.974557 loss_std 7.00309 loss_min 0.094917655 loss_max 28.55258\n",
      "32800 loss_mean 13.503325 loss_std 6.7436504 loss_min 2.2548194 loss_max 46.48053\n",
      "32900 loss_mean 12.463505 loss_std 7.307013 loss_min 0.36621875 loss_max 32.69858\n",
      "33000 loss_mean 13.785506 loss_std 6.6707735 loss_min 0.8595525 loss_max 30.885416\n",
      "['<bos>', ',', ',', ',', ',', ',', ',', ',', 'that', ',', ',']\n",
      "33100 loss_mean 12.994854 loss_std 5.7243567 loss_min 2.032845 loss_max 26.94294\n",
      "33200 loss_mean 13.16506 loss_std 6.9167 loss_min 0.0658597 loss_max 32.493263\n",
      "33300 loss_mean 12.634835 loss_std 7.1670356 loss_min 0.10784772 loss_max 33.272057\n",
      "33400 loss_mean 12.511135 loss_std 6.785247 loss_min 1.9632236 loss_max 34.95168\n",
      "33500 loss_mean 12.060497 loss_std 5.96217 loss_min 0.09266202 loss_max 29.950735\n",
      "33600 loss_mean 12.402069 loss_std 5.9111342 loss_min 0.3910607 loss_max 28.370424\n",
      "33700 loss_mean 12.78393 loss_std 6.349555 loss_min 0.04420109 loss_max 29.304806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33800 loss_mean 12.807307 loss_std 6.899493 loss_min 0.5586353 loss_max 28.117334\n",
      "33900 loss_mean 11.740256 loss_std 6.3129935 loss_min 0.11103764 loss_max 27.306847\n",
      "34000 loss_mean 12.362658 loss_std 6.6178865 loss_min 0.16476645 loss_max 35.926037\n",
      "['<bos>', ',', \"'s\", '<eos>', 'I', 'were', 'as', ',', 'were', 'I', '<unk>']\n",
      "34100 loss_mean 12.683462 loss_std 5.955844 loss_min 0.4034228 loss_max 29.793713\n",
      "34200 loss_mean 13.453871 loss_std 6.367729 loss_min 0.17687216 loss_max 28.671017\n",
      "34300 loss_mean 12.569189 loss_std 5.946906 loss_min 0.009566019 loss_max 26.371254\n",
      "34400 loss_mean 12.5308895 loss_std 6.269864 loss_min 0.24361321 loss_max 29.955656\n",
      "34500 loss_mean 11.910586 loss_std 6.0561185 loss_min 0.11476501 loss_max 32.80216\n",
      "34600 loss_mean 13.550953 loss_std 6.960074 loss_min 0.15983522 loss_max 36.30722\n",
      "34700 loss_mean 12.2127495 loss_std 6.4776673 loss_min 0.060363054 loss_max 26.553928\n",
      "34800 loss_mean 13.208958 loss_std 6.5603733 loss_min 0.046347998 loss_max 30.801651\n",
      "34900 loss_mean 12.713135 loss_std 6.735755 loss_min 0.3048002 loss_max 31.645329\n",
      "35000 loss_mean 12.327811 loss_std 6.4956536 loss_min 0.5302347 loss_max 32.36808\n",
      "['<bos>', ',', ',', ',', ',', ',', ',', ',', 'that', ',', 'to']\n",
      "35100 loss_mean 13.715641 loss_std 7.1899357 loss_min 1.6733705 loss_max 39.870903\n",
      "35200 loss_mean 13.327544 loss_std 7.8124766 loss_min 0.037021153 loss_max 36.031105\n",
      "35300 loss_mean 13.322462 loss_std 6.013942 loss_min 0.79653287 loss_max 27.522831\n",
      "35400 loss_mean 13.414088 loss_std 7.1734905 loss_min 0.5571682 loss_max 32.532604\n",
      "35500 loss_mean 12.604456 loss_std 7.0643 loss_min 0.88493705 loss_max 29.83017\n",
      "35600 loss_mean 12.672615 loss_std 5.693531 loss_min 0.06988704 loss_max 29.608381\n",
      "35700 loss_mean 12.27451 loss_std 7.10014 loss_min 0.04257884 loss_max 33.773228\n",
      "35800 loss_mean 13.868967 loss_std 7.026533 loss_min 0.0650766 loss_max 32.676403\n",
      "35900 loss_mean 13.022937 loss_std 6.4780307 loss_min 0.4944597 loss_max 30.511375\n",
      "36000 loss_mean 13.9871435 loss_std 7.2900696 loss_min 0.69137543 loss_max 32.409676\n",
      "['<bos>', '<unk>', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "36100 loss_mean 13.11162 loss_std 5.7198443 loss_min 1.8131801 loss_max 30.085102\n",
      "36200 loss_mean 13.436041 loss_std 5.895769 loss_min 0.057988867 loss_max 34.366634\n",
      "36300 loss_mean 13.836123 loss_std 6.8767576 loss_min 0.11203961 loss_max 42.191673\n",
      "36400 loss_mean 13.446608 loss_std 7.156314 loss_min 0.6662482 loss_max 33.02433\n",
      "36500 loss_mean 12.321508 loss_std 7.817009 loss_min 0.08837225 loss_max 51.542606\n",
      "36600 loss_mean 13.002411 loss_std 7.0809627 loss_min 0.16052338 loss_max 39.382095\n",
      "36700 loss_mean 12.45225 loss_std 7.247392 loss_min 0.4647699 loss_max 37.12874\n",
      "36800 loss_mean 13.22742 loss_std 6.78908 loss_min 0.2640289 loss_max 28.379772\n",
      "36900 loss_mean 13.059198 loss_std 6.841999 loss_min 0.5219628 loss_max 29.652424\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e035d6f7ac3549a3a21ec482595b48ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12306), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37000 loss_mean 13.958851 loss_std 7.049064 loss_min 1.3451111 loss_max 31.178764\n",
      "['<bos>', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "37100 loss_mean 12.365937 loss_std 6.41912 loss_min 0.1784303 loss_max 37.067146\n",
      "37200 loss_mean 12.329975 loss_std 6.2329783 loss_min 0.42283463 loss_max 30.192047\n",
      "37300 loss_mean 13.355958 loss_std 7.134558 loss_min 0.042868145 loss_max 37.763535\n",
      "37400 loss_mean 12.493601 loss_std 5.9889216 loss_min 0.29457933 loss_max 29.749636\n",
      "37500 loss_mean 12.903881 loss_std 5.904778 loss_min 1.5073617 loss_max 27.758694\n",
      "37600 loss_mean 13.332389 loss_std 6.7638683 loss_min 0.07309822 loss_max 31.569733\n",
      "37700 loss_mean 12.658224 loss_std 6.6222157 loss_min 0.18387923 loss_max 30.764368\n",
      "37800 loss_mean 13.067207 loss_std 6.8078704 loss_min 0.5991127 loss_max 30.797268\n",
      "37900 loss_mean 12.935075 loss_std 6.0380573 loss_min 0.030503364 loss_max 32.64238\n",
      "38000 loss_mean 12.997198 loss_std 6.217144 loss_min 0.22607088 loss_max 30.47718\n",
      "['<bos>', '<eos>', '<eos>', '<eos>', ',', ',', ',', ',', '.', '.', 'is']\n",
      "38100 loss_mean 12.777416 loss_std 6.6560507 loss_min 0.26014954 loss_max 31.949442\n",
      "38200 loss_mean 13.570039 loss_std 6.454179 loss_min 0.39525306 loss_max 29.969982\n",
      "38300 loss_mean 12.861382 loss_std 6.6990705 loss_min 0.60861504 loss_max 31.294022\n",
      "38400 loss_mean 12.387805 loss_std 6.2901554 loss_min 0.015128455 loss_max 29.668797\n",
      "38500 loss_mean 12.301691 loss_std 6.239734 loss_min 0.10630708 loss_max 29.525005\n",
      "38600 loss_mean 12.791218 loss_std 6.9717712 loss_min 0.4312627 loss_max 33.799503\n",
      "38700 loss_mean 13.758415 loss_std 6.981232 loss_min 1.8231035 loss_max 34.693077\n",
      "38800 loss_mean 13.407539 loss_std 6.5266266 loss_min 0.19522083 loss_max 29.800785\n",
      "38900 loss_mean 13.360668 loss_std 6.4311233 loss_min 0.1800471 loss_max 28.855864\n",
      "39000 loss_mean 13.139013 loss_std 6.4684196 loss_min 1.2165325 loss_max 27.2494\n",
      "['<bos>', 'to', 'that', ',', ',', ',', ',', ',', '<unk>', 'that', ',']\n",
      "39100 loss_mean 14.158346 loss_std 6.597585 loss_min 0.6086416 loss_max 29.29488\n",
      "39200 loss_mean 13.946247 loss_std 6.632754 loss_min 0.13074914 loss_max 29.865763\n",
      "39300 loss_mean 13.301944 loss_std 6.7721624 loss_min 0.7564993 loss_max 41.016197\n",
      "39400 loss_mean 14.074181 loss_std 7.976128 loss_min 0.0650079 loss_max 36.49125\n",
      "39500 loss_mean 12.296714 loss_std 5.965434 loss_min 0.32933766 loss_max 27.319407\n",
      "39600 loss_mean 13.950567 loss_std 6.4667997 loss_min 0.39481956 loss_max 34.165024\n",
      "39700 loss_mean 13.301947 loss_std 6.7549133 loss_min 0.008799644 loss_max 29.263592\n",
      "39800 loss_mean 13.980409 loss_std 7.2048197 loss_min 0.11295941 loss_max 33.31253\n",
      "39900 loss_mean 13.224927 loss_std 6.4111767 loss_min 0.22463092 loss_max 30.996216\n",
      "40000 loss_mean 14.3296175 loss_std 5.7259374 loss_min 1.6349089 loss_max 33.255707\n",
      "['<bos>', '<eos>', '.', '<unk>', '<unk>', 'to', '.', '<unk>', '<unk>', '<unk>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "# !pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib torchtext\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "# Standard PyTorch imports\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib.layers import layer_norm\n",
    "import nn_utils\n",
    "\n",
    "# For plots\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "import tensorflow as tf\n",
    "real = 1\n",
    "BATCH_SIZE = 16\n",
    "#!conda install torchtext spacy\n",
    "# !python -m spacy download en\n",
    "# !python -m spacy download de\n",
    "def detect_end(choice, eos_token=None):\n",
    "    return choice.flatten()[0] == eos_token\n",
    "\n",
    "if real:\n",
    "  from torchtext import data\n",
    "  from torchtext import datasets\n",
    "  import tqdm\n",
    "  import re\n",
    "  import spacy\n",
    "\n",
    "  spacy_de = spacy.load('de')\n",
    "  spacy_en = spacy.load('en')\n",
    "\n",
    "  url = re.compile('(<url>.*</url>)')\n",
    "\n",
    "  def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "  def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "  # Testing IWSLT\n",
    "  DE = data.Field(\n",
    "      tokenize=tokenize_de,\n",
    "      init_token='<bos>',\n",
    "      eos_token='<eos>',\n",
    "      include_lengths=True)\n",
    "  EN = data.Field(\n",
    "      tokenize=tokenize_en,\n",
    "      init_token='<bos>',\n",
    "      eos_token='<eos>',\n",
    "      include_lengths=True)\n",
    "\n",
    "  train, val, test = datasets.IWSLT.splits(\n",
    "      exts=('.de', '.en'), fields=(DE, EN))\n",
    "\n",
    "  train_it = data.Iterator(\n",
    "      train,\n",
    "      batch_size=BATCH_SIZE,\n",
    "      sort_within_batch=True,\n",
    "      train=True,\n",
    "      repeat=False,\n",
    "      shuffle=True)\n",
    "  MIN_WORD_FREQ = 10\n",
    "  MAX_NUM_WORDS = 10000\n",
    "  DE.build_vocab(train.src, min_freq=MIN_WORD_FREQ, max_size=MAX_NUM_WORDS)\n",
    "  EN.build_vocab(train.trg, min_freq=MIN_WORD_FREQ, max_size=MAX_NUM_WORDS)\n",
    "\n",
    "  num_wds_input = len(DE.vocab.itos)\n",
    "  num_wds_output = len(EN.vocab.itos)\n",
    "else:\n",
    "  num_wds_input = 1004\n",
    "\n",
    "\n",
    "class masked_softmax:\n",
    "  def __init__(self, v, mask, dim=2):\n",
    "    #bs, query dimension, key dimension\n",
    "    v_mask = v * mask\n",
    "    v_max = tf.reduce_max(v_mask, dim, keep_dims=True)\n",
    "    v_stable = v_mask - v_max\n",
    "\n",
    "    v_exp = tf.exp(v_stable) * mask\n",
    "    v_exp_sum = tf.reduce_sum(v_exp, dim, keep_dims=True)\n",
    "    self.v_mask, self.v_max, self.v_stable, self.v_exp, self.v_exp_sum = \\\n",
    "        v_mask, v_max, v_stable, v_exp, v_exp_sum\n",
    "    self.output = v_exp / (v_exp_sum + 1e-20)\n",
    "\n",
    "\n",
    "class Encoder:\n",
    "  def __init__(self, num_wds, wd_ind, mask, ndims=64, n_layers=6):\n",
    "    self.num_wds = num_wds\n",
    "    self.wd_ind = wd_ind\n",
    "    self.mask = mask\n",
    "    self.length = tf.shape(self.wd_ind)[1]\n",
    "    self.wd_emb = tf.Variable(\n",
    "        tf.random_uniform([self.num_wds, ndims], minval=-1, maxval=1.))\n",
    "    self.wd_vec = tf.nn.embedding_lookup(self.wd_emb, wd_ind)\n",
    "    self.pos = tf.reshape(\n",
    "        tf.range(tf.cast(self.length, tf.float32), dtype=tf.float32),\n",
    "        (1, -1, 1))\n",
    "    self.divider_exponent = tf.reshape(\n",
    "        tf.range(tf.cast(ndims // 2, tf.float32)),\n",
    "        (1, 1, -1)) * 2. / tf.cast(ndims, tf.float32)\n",
    "    self.divider = tf.pow(10000., self.divider_exponent)\n",
    "    self.input_to_sinusoids = self.pos / self.divider\n",
    "    self.pos_sin = tf.sin(self.input_to_sinusoids)\n",
    "    self.pos_cos = tf.cos(self.input_to_sinusoids)\n",
    "    # self.position = tf.reshape(\n",
    "    #     tf.range(tf.cast(self.length, tf.float32), dtype=tf.float32) / 10000,\n",
    "    #     (1, -1, 1))\n",
    "    self.position = tf.concat((self.pos_sin, self.pos_cos), -1)\n",
    "    self.w_tilde = embedding = self.wd_vec + self.position\n",
    "    self.encoding = []\n",
    "    self.attentionLayers = []\n",
    "    for _ in range(n_layers):\n",
    "      attentionLayer = AttentionLayer(embedding, mask)\n",
    "      embedding = attentionLayer.output\n",
    "      self.encoding.append(embedding)\n",
    "      self.attentionLayers.append(attentionLayer)\n",
    "\n",
    "\n",
    "class AttentionLayer:\n",
    "  def __init__(self, X, mask, X_decode=None, decode_mask=None, ff_layer=True):\n",
    "    bs, length, ndim = [v.value for v in X.shape]\n",
    "    self.X = X\n",
    "    if X_decode is None:\n",
    "      self.q, self.k, self.v = [\n",
    "          tf.tanh(tf.layers.dense(X, ndim)) for _ in range(3)\n",
    "      ]\n",
    "      decode_mask = mask\n",
    "    else:\n",
    "      self.q = tf.tanh(tf.layers.dense(X_decode, ndim))\n",
    "      self.k, self.v = [tf.tanh(tf.layers.dense(X, ndim)) for _ in range(2)]\n",
    "    #batch, attention queries, attention keys, embeddings\n",
    "    self.q_expanded = tf.expand_dims(self.q, 2)\n",
    "    self.k_expanded = tf.expand_dims(self.k, 1)\n",
    "    self.v_expanded = tf.expand_dims(self.v, 1)\n",
    "    self.s_raw = tf.reduce_sum(self.q_expanded * self.k_expanded, -1)\n",
    "    self.mask = tf.expand_dims(decode_mask, 2) * tf.expand_dims(mask, 1)\n",
    "    self.masked_softmax = masked_softmax(self.s_raw, self.mask)\n",
    "    self.s = self.masked_softmax.output\n",
    "    self.a = tf.expand_dims(self.s * self.mask, -1) * self.v_expanded\n",
    "    #A is shape bs, query, key, emb\n",
    "    self.a_compressed = tf.reduce_sum(self.a, 2)\n",
    "    if X_decode is None:\n",
    "      self.e = layer_norm(self.a_compressed + X)\n",
    "    else:\n",
    "      self.e = layer_norm(self.a_compressed + X_decode)\n",
    "    if ff_layer:\n",
    "      self.output = layer_norm(tf.layers.dense(self.e, ndim) + self.e)\n",
    "    else:\n",
    "      self.output = self.e\n",
    "\n",
    "\n",
    "class Decoder:\n",
    "  def __init__(self, num_wds, wd_ind, mask, encoder, ndims=20, n_layers=6):\n",
    "    self.num_wds = num_wds\n",
    "    self.wd_ind = wd_ind\n",
    "    self.mask = mask\n",
    "    self.encoder = encoder\n",
    "    self.length = tf.shape(self.wd_ind)[1]\n",
    "    self.wd_emb = tf.Variable(\n",
    "        tf.random_uniform([self.num_wds, ndims], minval=-1, maxval=1.))\n",
    "    self.wd_vec = tf.nn.embedding_lookup(self.wd_emb, wd_ind)\n",
    "    self.pos = tf.reshape(\n",
    "        tf.range(tf.cast(self.length, tf.float32), dtype=tf.float32),\n",
    "        (1, -1, 1))\n",
    "    self.divider_exponent = tf.reshape(\n",
    "        tf.range(tf.cast(ndims // 2, tf.float32)),\n",
    "        (1, 1, -1)) * 2. / tf.cast(ndims, tf.float32)\n",
    "    self.divider = tf.pow(10000., self.divider_exponent)\n",
    "    self.input_to_sinusoids = self.pos / self.divider\n",
    "    self.pos_sin = tf.sin(self.input_to_sinusoids)\n",
    "    self.pos_cos = tf.cos(self.input_to_sinusoids)\n",
    "    # self.position = tf.reshape(\n",
    "    #     tf.range(tf.cast(self.length, tf.float32), dtype=tf.float32) / 10000,\n",
    "    #     (1, -1, 1))\n",
    "    self.position = tf.concat((self.pos_sin, self.pos_cos), -1)\n",
    "    self.w_tilde = embedding = self.wd_vec + self.position\n",
    "    self.decoding = []\n",
    "    self.self_attentions = []\n",
    "    self.encoder_attentions = []\n",
    "    self.early_outputs = []\n",
    "    for l_idx in range(n_layers):\n",
    "      attn = AttentionLayer(embedding, mask, ff_layer=False)\n",
    "      self.self_attentions.append(attn)\n",
    "      encode_attn = AttentionLayer(encoder.encoding[l_idx], encoder.mask,\n",
    "                                   attn.output, mask)\n",
    "      self.encoder_attentions.append(encode_attn)\n",
    "      embedding = encode_attn.output\n",
    "      if l_idx < n_layers - 1:\n",
    "        early_output = tf.layers.dense(embedding, num_wds)\n",
    "        #early_output_masked = masked_softmax(early_output, tf.expand_dims(mask, -1), dim=2).output\n",
    "        self.early_outputs.append(early_output)\n",
    "\n",
    "    self.output_raw = tf.layers.dense(embedding, num_wds)\n",
    "    #bs, word in sentence of target, embedding\n",
    "\n",
    "    self.outsoftmax = masked_softmax(self.output_raw, tf.expand_dims(mask, -1), dim=2)\n",
    "    self.output = self.outsoftmax.output\n",
    "\n",
    "class Transformer:\n",
    "  def __init__(self, num_wds):\n",
    "    self.num_wds = num_wds\n",
    "    n_layers = 6\n",
    "    ndims = 256\n",
    "    self.learning_rate = tf.placeholder(tf.float32, None)\n",
    "    self.wd_ind_src = wd_ind_src = tf.placeholder(tf.int32, (None, None))\n",
    "    self.wd_ind_trg = wd_ind_trg = tf.placeholder(tf.int32, (None, None))\n",
    "    self.input_lengths = tf.placeholder(tf.int32, [None])\n",
    "    self.output_lengths = tf.placeholder(tf.int32, [None])\n",
    "    self.input_mask = tf.sequence_mask(\n",
    "        self.input_lengths,\n",
    "        maxlen=tf.shape(self.wd_ind_src)[-1],\n",
    "        dtype=tf.float32)\n",
    "    self.output_mask = tf.sequence_mask(\n",
    "        self.output_lengths,\n",
    "        maxlen=tf.shape(self.wd_ind_trg)[-1],\n",
    "        dtype=tf.float32)\n",
    "    self.encoder = Encoder(\n",
    "        num_wds, wd_ind_src, self.input_mask, n_layers=n_layers, ndims=ndims)\n",
    "    self.decoder = Decoder(\n",
    "        num_wds,\n",
    "        wd_ind_trg,\n",
    "        self.output_mask,\n",
    "        self.encoder,\n",
    "        n_layers=n_layers,\n",
    "        ndims=ndims)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "    self.prediction_mask = tf.concat((tf.zeros(\n",
    "        (tf.shape(self.output_mask)[0], 1)), self.output_mask[:, :-1] - self.output_mask[:, 1:]),\n",
    "                                     1)\n",
    "    self.losses = tf.reduce_mean([tf.reduce_mean(\n",
    "        tf.square(\n",
    "            tf.reduce_max(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=self.wd_ind_trg, logits=logits) *\n",
    "                self.prediction_mask, 1))) for logits in self.decoder.early_outputs + [self.decoder.output_raw]])\n",
    "    self.loss = tf.reduce_mean(\n",
    "        tf.square(\n",
    "            tf.reduce_max(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=self.wd_ind_trg, logits=self.decoder.output_raw) *\n",
    "                self.prediction_mask, 1)))\n",
    "    self.optimizer, self.grad_norm_total = nn_utils.apply_clipped_optimizer(\n",
    "        opt, self.losses)\n",
    "\n",
    "\n",
    "# In[71]:\n",
    "\n",
    "transformer = Transformer(num_wds_input)\n",
    "MAX_LEN = 20\n",
    "\n",
    "\n",
    "def predict_one(src_tensor, src_len, trg_tensor):\n",
    "    NUM_MAX_DECODE = 5 #MAX_LEN\n",
    "    src_len_decode = src_len[0:1]\n",
    "    src_decode = src_tensor[0:1, :src_len_decode[0]]\n",
    "    autoregressive = trg_tensor[0:1, 0:1]\n",
    "    for dec_idx in range(10):\n",
    "        pred = sess.run(\n",
    "            transformer.decoder.outsoftmax.output[:,-1,:], {\n",
    "                transformer.wd_ind_src: src_decode,\n",
    "                transformer.input_lengths: src_len_decode,\n",
    "                transformer.wd_ind_trg: autoregressive,\n",
    "                transformer.output_lengths: np.ones(1)*autoregressive.shape[1],\n",
    "            })\n",
    "        choice = pred.argmax(1)\n",
    "        autoregressive = np.concatenate((autoregressive, np.expand_dims(choice, 0)), -1)\n",
    "        if detect_end(choice, None):\n",
    "            break\n",
    "    translation = [EN.vocab.itos[a] for a in autoregressive.flatten()]\n",
    "    print(translation)\n",
    "    \n",
    "    \n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "itr = 0\n",
    "print_freq = 100\n",
    "running_losses = []\n",
    "if real:\n",
    "  #for itr, train_batch in enumerate(tqdm.tqdm_notebook(train_it)):\n",
    "  for ep in range(100):\n",
    "      for train_batch in tqdm.tqdm_notebook(train_it):\n",
    "        itr += 1\n",
    "        src_tensor = train_batch.src[0].data.cpu().numpy().transpose()\n",
    "        src_len = train_batch.src[1].cpu().numpy()\n",
    "        trg_tensor = train_batch.trg[0].data.cpu().numpy().transpose()\n",
    "        trg_len = train_batch.trg[1].cpu().numpy()\n",
    "        src_tensor, trg_tensor = [t[:, :MAX_LEN] for t in [src_tensor, trg_tensor]]\n",
    "        src_len, trg_len = [np.clip(t, 0, MAX_LEN) for t in [src_len, trg_len]]\n",
    "        trg_len = np.ceil(\n",
    "            np.random.uniform(size=trg_len.shape[0]) * (trg_len - 1)).astype(int)\n",
    "        trn_feed_dict = {\n",
    "            transformer.wd_ind_src: src_tensor,\n",
    "            transformer.input_lengths: src_len,\n",
    "            transformer.wd_ind_trg: trg_tensor,\n",
    "            transformer.output_lengths: trg_len,\n",
    "            transformer.learning_rate: 1e-2 / (np.sqrt(itr + 3))\n",
    "        }\n",
    "        _, loss = sess.run([transformer.optimizer, transformer.loss],\n",
    "                           trn_feed_dict)\n",
    "        running_losses.append(loss)\n",
    "        if itr % print_freq == 0:\n",
    "          running_losses = np.array(running_losses)\n",
    "          print(itr, 'loss_mean', running_losses.mean(), 'loss_std', running_losses.std(), 'loss_min', running_losses.min(),\n",
    "              'loss_max', running_losses.max())\n",
    "          running_losses = []\n",
    "          if itr % 1000 == 0:\n",
    "            predict_one(src_tensor, src_len, trg_tensor)\n",
    "        if itr > 2000000:\n",
    "          break\n",
    "else:\n",
    "\n",
    "  src_tensor = np.random.randint(low=0, high=num_wds_input, size=(BATCH_SIZE, 81))\n",
    "  src_len = np.random.randint(2, 81, BATCH_SIZE)\n",
    "  trg_tensor = np.random.randint(low=0, high=num_wds_input, size=(BATCH_SIZE, 84))\n",
    "  trg_len = np.random.randint(2, 84, BATCH_SIZE)\n",
    "\n",
    "  fd = {\n",
    "      transformer.wd_ind_src: src_tensor,\n",
    "      transformer.wd_ind_trg: trg_tensor,\n",
    "      transformer.input_lengths: src_len,\n",
    "      transformer.output_lengths: trg_len,\n",
    "      transformer.learning_rate: 1e-2}\n",
    "  sess.run([transformer.optimizer, transformer.loss], fd)\n",
    "# In[75]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MAX_DECODE = MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "src_len_decode = src_len[0:1]\n",
    "src_decode = src_tensor[0:1, :src_len_decode[0]]\n",
    "autoregressive = trg_tensor[0:1, 0:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dec_idx in range(10):\n",
    "    pred = sess.run(\n",
    "        transformer.decoder.outsoftmax.output[:,-1,:], {\n",
    "            transformer.wd_ind_src: src_decode,\n",
    "            transformer.input_lengths: src_len_decode,\n",
    "            transformer.wd_ind_trg: autoregressive,\n",
    "            transformer.output_lengths: np.ones(1)*autoregressive.shape[1],\n",
    "        })\n",
    "    choice = pred.argmax(1)\n",
    "    autoregressive = np.concatenate((autoregressive, np.expand_dims(choice, 0)), -1)\n",
    "    if detect_end(choice, None):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = [EN.vocab.itos[a] for a in autoregressive.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[:,12:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
