{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3196ddf14f849c387f61a3918813da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loss_mean 84.57846 loss_std 0.0 loss_min 84.57846 loss_max 84.57846\n",
      "2 loss_mean 82.0095 loss_std 0.0 loss_min 82.0095 loss_max 82.0095\n",
      "3 loss_mean 103.72043 loss_std 0.0 loss_min 103.72043 loss_max 103.72043\n",
      "4 loss_mean 52.54789 loss_std 0.0 loss_min 52.54789 loss_max 52.54789\n",
      "10 loss_mean 60.01445 loss_std 6.029169 loss_min 52.2023 loss_max 70.08912\n",
      "20 loss_mean 51.405678 loss_std 4.52098 loss_min 41.711365 loss_max 59.912785\n",
      "30 loss_mean 46.558437 loss_std 2.2536125 loss_min 43.153755 loss_max 51.485588\n",
      "40 loss_mean 47.571346 loss_std 4.1877704 loss_min 41.815277 loss_max 55.278114\n",
      "50 loss_mean 45.561466 loss_std 4.256003 loss_min 40.166565 loss_max 53.251667\n",
      "60 loss_mean 46.226063 loss_std 5.6653423 loss_min 37.126755 loss_max 54.988613\n",
      "70 loss_mean 44.585926 loss_std 6.752393 loss_min 31.409367 loss_max 55.0481\n",
      "80 loss_mean 44.872345 loss_std 2.6572459 loss_min 40.076492 loss_max 50.08348\n",
      "90 loss_mean 44.51847 loss_std 5.3046503 loss_min 34.225582 loss_max 50.843494\n",
      "100 loss_mean 44.616802 loss_std 4.9607015 loss_min 35.894173 loss_max 54.820015\n",
      "['<bos>', 'es', 'ist', 'wahr', ',', 'wenn', 'sie', 'also', 'dieses', 'spiel', 'mit', 'mir', 'heute', 'für', 'nur', 'eine', 'minute', 'spielen', ',', 'werden']\n",
      "['<bos>', 'this', 'is', 'true', ',', 'so', 'if', 'you', 'play', 'this', 'game', 'with', 'me', 'today']\n",
      "['<bos>', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you']\n",
      "200 loss_mean 44.027203 loss_std 4.341602 loss_min 33.135254 loss_max 55.16786\n",
      "['<bos>', 'er', 'war', 'sehr', '<unk>', 'und', 'irgendwie', 'teil', 'des', '<unk>', 'von', 'großbritannien', ',', 'und', 'an', 'einem', '<unk>', 'in', '<unk>', 'wären']\n",
      "['<bos>', 'he', 'was', 'a', 'very', 'wealthy', 'man', ',', 'and', 'a', 'sort', 'of', ',', 'part']\n",
      "['<bos>', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you']\n",
      "300 loss_mean 42.561813 loss_std 4.172245 loss_min 31.98293 loss_max 52.280468\n",
      "['<bos>', 'jedoch', 'ist', 'es', 'nun', 'an', 'der', 'zeit', 'zu', 'erkennen', ',', 'dass', 'wir', 'an', 'die', 'ökologischen', 'grenzen', 'unseres', 'planeten', 'stoßen']\n",
      "['<bos>', 'but', 'what', 'we', 'have', 'to', 'recognize', 'now', 'is', 'that', 'we', 'are']\n",
      "['<bos>', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1f24f511f8e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    316\u001b[0m         }\n\u001b[1;32m    317\u001b[0m         _, loss = sess.run([transformer.optimizer, transformer.loss],\n\u001b[0;32m--> 318\u001b[0;31m                            trn_feed_dict)\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0mrunning_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "# !pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib torchtext\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "# Standard PyTorch imports\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib.layers import layer_norm\n",
    "import nn_utils\n",
    "\n",
    "# For plots\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "import tensorflow as tf\n",
    "real = 1\n",
    "BATCH_SIZE = 64\n",
    "#!conda install torchtext spacy\n",
    "# !python -m spacy download en\n",
    "# !python -m spacy download de\n",
    "def detect_end(choice, eos_token=None):\n",
    "    return choice.flatten()[0] == eos_token\n",
    "\n",
    "if real:\n",
    "  from torchtext import data\n",
    "  from torchtext import datasets\n",
    "  import tqdm\n",
    "  import re\n",
    "  import spacy\n",
    "\n",
    "  spacy_de = spacy.load('de')\n",
    "  spacy_en = spacy.load('en')\n",
    "\n",
    "  url = re.compile('(<url>.*</url>)')\n",
    "\n",
    "  def tokenize_de(text):\n",
    "    return [tok.text.lower() for tok in spacy_de.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "  def tokenize_en(text):\n",
    "    return [tok.text.lower() for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "  # Testing IWSLT\n",
    "  DE = data.Field(\n",
    "      tokenize=tokenize_de,\n",
    "      init_token='<bos>',\n",
    "      eos_token='<eos>',\n",
    "      include_lengths=True)\n",
    "  EN = data.Field(\n",
    "      tokenize=tokenize_en,\n",
    "      init_token='<bos>',\n",
    "      eos_token='<eos>',\n",
    "      include_lengths=True)\n",
    "\n",
    "  train, val, test = datasets.IWSLT.splits(\n",
    "      exts=('.de', '.en'), fields=(DE, EN))\n",
    "\n",
    "  train_it = data.Iterator(\n",
    "      train,\n",
    "      batch_size=BATCH_SIZE,\n",
    "      sort_within_batch=True,\n",
    "      train=True,\n",
    "      repeat=False,\n",
    "      shuffle=True)\n",
    "  MIN_WORD_FREQ = 10\n",
    "  MAX_NUM_WORDS = 10000\n",
    "  DE.build_vocab(train.src, min_freq=MIN_WORD_FREQ, max_size=MAX_NUM_WORDS)\n",
    "  EN.build_vocab(train.trg, min_freq=MIN_WORD_FREQ, max_size=MAX_NUM_WORDS)\n",
    "\n",
    "  num_wds_input = len(DE.vocab.itos)\n",
    "  num_wds_output = len(EN.vocab.itos)\n",
    "else:\n",
    "  num_wds_input = 1004\n",
    "\n",
    "\n",
    "class masked_softmax:\n",
    "  def __init__(self, v, mask, dim=2):\n",
    "    #bs, query dimension, key dimension\n",
    "    v_mask = v * mask\n",
    "    v_max = tf.reduce_max(v_mask, dim, keep_dims=True)\n",
    "    v_stable = v_mask - v_max\n",
    "\n",
    "    v_exp = tf.exp(v_stable) * mask\n",
    "    v_exp_sum = tf.reduce_sum(v_exp, dim, keep_dims=True)\n",
    "    self.v_mask, self.v_max, self.v_stable, self.v_exp, self.v_exp_sum = \\\n",
    "        v_mask, v_max, v_stable, v_exp, v_exp_sum\n",
    "    self.output = v_exp / (v_exp_sum + 1e-20)\n",
    "\n",
    "\n",
    "class Encoder:\n",
    "  def __init__(self, num_wds, wd_ind, mask, ndims=64, n_layers=6):\n",
    "    self.num_wds = num_wds\n",
    "    self.wd_ind = wd_ind\n",
    "    self.mask = mask\n",
    "    self.length = tf.shape(self.wd_ind)[1]\n",
    "    self.wd_emb = tf.Variable(\n",
    "        tf.random_uniform([self.num_wds, ndims], minval=-1, maxval=1.))\n",
    "    self.wd_vec = tf.nn.embedding_lookup(self.wd_emb, wd_ind)\n",
    "    self.pos = tf.reshape(\n",
    "        tf.range(tf.cast(self.length, tf.float32), dtype=tf.float32),\n",
    "        (1, -1, 1))\n",
    "    self.divider_exponent = tf.reshape(\n",
    "        tf.range(tf.cast(ndims // 2, tf.float32)),\n",
    "        (1, 1, -1)) * 2. / tf.cast(ndims, tf.float32)\n",
    "    self.divider = tf.pow(10000., self.divider_exponent)\n",
    "    self.input_to_sinusoids = self.pos / self.divider\n",
    "    self.pos_sin = tf.sin(self.input_to_sinusoids)\n",
    "    self.pos_cos = tf.cos(self.input_to_sinusoids)\n",
    "    # self.position = tf.reshape(\n",
    "    #     tf.range(tf.cast(self.length, tf.float32), dtype=tf.float32) / 10000,\n",
    "    #     (1, -1, 1))\n",
    "    self.position = tf.concat((self.pos_sin, self.pos_cos), -1)\n",
    "    self.w_tilde = embedding = self.wd_vec + self.position\n",
    "    self.encoding = []\n",
    "    self.attentionLayers = []\n",
    "    for _ in range(n_layers):\n",
    "      attentionLayer = AttentionLayer(embedding, mask)\n",
    "      embedding = attentionLayer.output\n",
    "      self.encoding.append(embedding)\n",
    "      self.attentionLayers.append(attentionLayer)\n",
    "\n",
    "class AttentionLayer:\n",
    "    def __init__(self, X, mask, X_decode=None, decode_mask=None, ff_layer=True):\n",
    "        bs, length, ndim = [v.value for v in X.shape]\n",
    "        self.layers = layers = [\n",
    "            AttentionSubLayer(\n",
    "                X, mask, X_decode=X_decode, decode_mask=decode_mask, ff_layer=ff_layer)\n",
    "            for _ in range(4)]\n",
    "        self.sub_outputs = [l.a_compressed for l in layers]\n",
    "        self.a_compressed = tf.concat((self.sub_outputs), -1)\n",
    "        self.linearafterattn = tf.layers.dense(self.a_compressed, ndim)\n",
    "        if X_decode is None:\n",
    "          self.e = layer_norm(self.linearafterattn + X)\n",
    "        else:\n",
    "          self.e = layer_norm(self.linearafterattn + X_decode)\n",
    "        if ff_layer:\n",
    "          self.output = layer_norm(tf.layers.dense(tf.nn.leaky_relu(\n",
    "              tf.layers.dense(self.e, ndim)),ndim) + self.e)\n",
    "        else:\n",
    "          self.output = self.e\n",
    "    \n",
    "class AttentionSubLayer:\n",
    "  def __init__(self, X, mask, X_decode=None, decode_mask=None, ff_layer=True):\n",
    "    bs, length, ndim = [v.value for v in X.shape]\n",
    "    ndim = ndim // 4\n",
    "    self.X = X\n",
    "    if X_decode is None:\n",
    "      self.q, self.k, self.v = [\n",
    "          tf.tanh(tf.layers.dense(X, ndim)) for _ in range(3)\n",
    "      ]\n",
    "      decode_mask = mask\n",
    "    else:\n",
    "      self.q = tf.tanh(tf.layers.dense(X_decode, ndim))\n",
    "      self.k, self.v = [tf.tanh(tf.layers.dense(X, ndim)) for _ in range(2)]\n",
    "    #batch, attention queries, attention keys, embeddings\n",
    "    self.q_expanded = tf.expand_dims(self.q, 2)\n",
    "    self.k_expanded = tf.expand_dims(self.k, 1)\n",
    "    self.v_expanded = tf.expand_dims(self.v, 1)\n",
    "    self.s_raw = tf.reduce_sum(self.q_expanded * self.k_expanded, -1)/np.sqrt(ndim)\n",
    "    self.mask = tf.expand_dims(decode_mask, 2) * tf.expand_dims(mask, 1)\n",
    "    self.masked_softmax = masked_softmax(self.s_raw, self.mask)\n",
    "    self.s = self.masked_softmax.output\n",
    "    self.a = tf.expand_dims(self.s * self.mask, -1) * self.v_expanded\n",
    "    #A is shape bs, query, key, emb\n",
    "    self.a_compressed = tf.reduce_sum(self.a, 2)\n",
    "\n",
    "\n",
    "class Decoder:\n",
    "  def __init__(self, num_wds, wd_ind, mask, encoder, ndims=20, n_layers=6):\n",
    "    self.num_wds = num_wds\n",
    "    self.wd_ind = wd_ind\n",
    "    self.mask = mask\n",
    "    self.encoder = encoder\n",
    "    self.length = tf.shape(self.wd_ind)[1]\n",
    "    self.wd_emb = tf.Variable(\n",
    "        tf.random_uniform([self.num_wds, ndims], minval=-1, maxval=1.))\n",
    "    self.wd_vec = tf.nn.embedding_lookup(self.wd_emb, wd_ind)\n",
    "    self.pos = tf.reshape(\n",
    "        tf.range(tf.cast(self.length, tf.float32), dtype=tf.float32),\n",
    "        (1, -1, 1))\n",
    "    self.divider_exponent = tf.reshape(\n",
    "        tf.range(tf.cast(ndims // 2, tf.float32)),\n",
    "        (1, 1, -1)) * 2. / tf.cast(ndims, tf.float32)\n",
    "    self.divider = tf.pow(10000., self.divider_exponent)\n",
    "    self.input_to_sinusoids = self.pos / self.divider\n",
    "    self.pos_sin = tf.sin(self.input_to_sinusoids)\n",
    "    self.pos_cos = tf.cos(self.input_to_sinusoids)\n",
    "    self.position = tf.concat((self.pos_sin, self.pos_cos), -1)\n",
    "    self.w_tilde = embedding = self.wd_vec + self.position\n",
    "    self.decoding = []\n",
    "    self.self_attentions = []\n",
    "    self.encoder_attentions = []\n",
    "    self.early_outputs = []\n",
    "    for l_idx in range(n_layers):\n",
    "      attn = AttentionLayer(embedding, mask, ff_layer=False)\n",
    "      self.self_attentions.append(attn)\n",
    "      encode_attn = AttentionLayer(encoder.encoding[l_idx], encoder.mask,\n",
    "                                   attn.output, mask)\n",
    "      self.encoder_attentions.append(encode_attn)\n",
    "      embedding = encode_attn.output\n",
    "      if l_idx < n_layers - 1:\n",
    "        early_output = tf.layers.dense(embedding, num_wds)\n",
    "        self.early_outputs.append(early_output)\n",
    "\n",
    "    self.output_raw = tf.layers.dense(embedding, num_wds)\n",
    "\n",
    "    self.outsoftmax = masked_softmax(self.output_raw, tf.expand_dims(mask, -1), dim=2)\n",
    "    self.output = self.outsoftmax.output\n",
    "\n",
    "class Transformer:\n",
    "  def __init__(self, num_wds):\n",
    "    self.num_wds = num_wds\n",
    "    n_layers = 6\n",
    "    ndims = 256\n",
    "    self.learning_rate = tf.placeholder(tf.float32, None)\n",
    "    self.wd_ind_src = wd_ind_src = tf.placeholder(tf.int32, (None, None))\n",
    "    self.wd_ind_trg = wd_ind_trg = tf.placeholder(tf.int32, (None, None))\n",
    "    self.input_lengths = tf.placeholder(tf.int32, [None])\n",
    "    self.output_lengths = tf.placeholder(tf.int32, [None])\n",
    "    self.input_mask = tf.sequence_mask(\n",
    "        self.input_lengths,\n",
    "        maxlen=tf.shape(self.wd_ind_src)[-1],\n",
    "        dtype=tf.float32)\n",
    "    self.output_mask = tf.sequence_mask(\n",
    "        self.output_lengths,\n",
    "        maxlen=tf.shape(self.wd_ind_trg)[-1],\n",
    "        dtype=tf.float32)\n",
    "    self.encoder = Encoder(\n",
    "        num_wds, wd_ind_src, self.input_mask, n_layers=n_layers, ndims=ndims)\n",
    "    self.decoder = Decoder(\n",
    "        num_wds,\n",
    "        wd_ind_trg,\n",
    "        self.output_mask,\n",
    "        self.encoder,\n",
    "        n_layers=n_layers,\n",
    "        ndims=ndims)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "    self.prediction_mask = tf.concat((tf.zeros(\n",
    "        (tf.shape(self.output_mask)[0], 1)), self.output_mask[:, :-1] - self.output_mask[:, 1:]),\n",
    "                                     1)\n",
    "    self.losses = tf.reduce_mean([tf.reduce_mean(\n",
    "        tf.square(\n",
    "            tf.reduce_max(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=self.wd_ind_trg, logits=logits) *\n",
    "                self.prediction_mask, 1))) for logits in self.decoder.early_outputs + [self.decoder.output_raw]])\n",
    "    self.loss = tf.reduce_mean(\n",
    "        tf.square(\n",
    "            tf.reduce_max(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=self.wd_ind_trg, logits=self.decoder.output_raw) *\n",
    "                self.prediction_mask, 1)))\n",
    "    self.optimizer, self.grad_norm_total = nn_utils.apply_clipped_optimizer(\n",
    "        opt, self.losses)\n",
    "\n",
    "\n",
    "# In[71]:\n",
    "\n",
    "transformer = Transformer(num_wds_input)\n",
    "MAX_LEN = 20\n",
    "\n",
    "\n",
    "def predict_one(src_tensor, src_len, trg_tensor):\n",
    "    NUM_MAX_DECODE = 8 #MAX_LEN\n",
    "    src_len_decode = src_len[0:1]\n",
    "    src_decode = src_tensor[0:1, :src_len_decode[0]]\n",
    "    autoregressive = trg_tensor[0:1, 0:1]\n",
    "    for dec_idx in range(NUM_MAX_DECODE):\n",
    "        pred = sess.run(\n",
    "            transformer.decoder.outsoftmax.output[:,-1,:], {\n",
    "                transformer.wd_ind_src: src_decode,\n",
    "                transformer.input_lengths: src_len_decode,\n",
    "                transformer.wd_ind_trg: autoregressive,\n",
    "                transformer.output_lengths: np.ones(1)*autoregressive.shape[1],\n",
    "            })\n",
    "        choice = pred.argmax(1)\n",
    "        autoregressive = np.concatenate((autoregressive, np.expand_dims(choice, 0)), -1)\n",
    "        if detect_end(choice, None):\n",
    "            break\n",
    "    translation = [EN.vocab.itos[a] for a in autoregressive.flatten()]\n",
    "    print(translation)\n",
    "\n",
    "def beamsearch(src_tensor, src_len, trg_tensor):\n",
    "    NUM_MAX_DECODE = 8 #MAX_LEN\n",
    "    src_len_decode = src_len[0:1]\n",
    "    src_decode = src_tensor[0:1, :src_len_decode[0]]\n",
    "    init_token = trg_tensor[0:1, 0:1]\n",
    "    for dec_idx in range(NUM_MAX_DECODE):\n",
    "        pred = sess.run(\n",
    "            transformer.decoder.outsoftmax.output[:,-1,:], {\n",
    "                transformer.wd_ind_src: src_decode,\n",
    "                transformer.input_lengths: src_len_decode,\n",
    "                transformer.wd_ind_trg: autoregressive,\n",
    "                transformer.output_lengths: np.ones(1)*autoregressive.shape[1],\n",
    "            })\n",
    "        choice = pred.argmax(1)\n",
    "        autoregressive = np.concatenate((autoregressive, np.expand_dims(choice, 0)), -1)\n",
    "        if detect_end(choice, None):\n",
    "            break\n",
    "    translation = [EN.vocab.itos[a] for a in autoregressive.flatten()]\n",
    "    print(translation)\n",
    "    \n",
    "def print_src(src_tensor, src_len, vocab):\n",
    "    src_len_decode = src_len[0:1]\n",
    "    src_decode = src_tensor[0:1, :src_len_decode[0]]\n",
    "    translation = [vocab.vocab.itos[a] for a in src_decode.flatten()]\n",
    "    print(translation)\n",
    "    \n",
    "    \n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "itr = 0\n",
    "print_freq = 100\n",
    "running_losses = []\n",
    "if real:\n",
    "  #for itr, train_batch in enumerate(tqdm.tqdm_notebook(train_it)):\n",
    "  for ep in range(100):\n",
    "      for train_batch in tqdm.tqdm_notebook(train_it):\n",
    "        itr += 1\n",
    "        src_tensor = train_batch.src[0].data.cpu().numpy().transpose()\n",
    "        src_len = train_batch.src[1].cpu().numpy()\n",
    "        trg_tensor = train_batch.trg[0].data.cpu().numpy().transpose()\n",
    "        trg_len = train_batch.trg[1].cpu().numpy()\n",
    "        src_tensor, trg_tensor = [t[:, :MAX_LEN] for t in [src_tensor, trg_tensor]]\n",
    "        src_len, trg_len = [np.clip(t, 0, MAX_LEN) for t in [src_len, trg_len]]\n",
    "        trg_len = np.ceil(\n",
    "            np.random.uniform(size=trg_len.shape[0]) * (trg_len - 1)).astype(int)\n",
    "        trn_feed_dict = {\n",
    "            transformer.wd_ind_src: src_tensor,\n",
    "            transformer.input_lengths: src_len,\n",
    "            transformer.wd_ind_trg: trg_tensor,\n",
    "            transformer.output_lengths: trg_len,\n",
    "            transformer.learning_rate: 1e-1 / (np.sqrt(itr + 3))\n",
    "        }\n",
    "        _, loss = sess.run([transformer.optimizer, transformer.loss],\n",
    "                           trn_feed_dict)\n",
    "        running_losses.append(loss)\n",
    "        if itr % print_freq == 0 or (itr < 5) or (itr < 100 and itr % 10 == 0):\n",
    "          running_losses = np.array(running_losses)\n",
    "          print(itr, 'loss_mean', running_losses.mean(), 'loss_std', running_losses.std(), 'loss_min', running_losses.min(),\n",
    "              'loss_max', running_losses.max())\n",
    "          running_losses = []\n",
    "          if itr % 100 == 0:\n",
    "            print_src(src_tensor, src_len, DE)\n",
    "            print_src(trg_tensor, trg_len, EN)\n",
    "            predict_one(src_tensor, src_len, trg_tensor)\n",
    "        if itr > 2000000:\n",
    "          break\n",
    "else:\n",
    "\n",
    "  src_tensor = np.random.randint(low=0, high=num_wds_input, size=(BATCH_SIZE, 81))\n",
    "  src_len = np.random.randint(2, 81, BATCH_SIZE)\n",
    "  trg_tensor = np.random.randint(low=0, high=num_wds_input, size=(BATCH_SIZE, 84))\n",
    "  trg_len = np.random.randint(2, 84, BATCH_SIZE)\n",
    "\n",
    "  fd = {\n",
    "      transformer.wd_ind_src: src_tensor,\n",
    "      transformer.wd_ind_trg: trg_tensor,\n",
    "      transformer.input_lengths: src_len,\n",
    "      transformer.output_lengths: trg_len,\n",
    "      transformer.learning_rate: 1e-2}\n",
    "  sess.run([transformer.optimizer, transformer.loss], fd)\n",
    "# In[75]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9976023 1.9868424] [array([[ 2, 16, 15]]), array([[ 2, 15, 15]])]\n",
      "[1.9910235 2.9802637] [array([[ 2, 16, 15, 15]]), array([[ 2, 15, 15, 15]])]\n",
      "['<bos>', 'you', 'you', 'you']\n"
     ]
    }
   ],
   "source": [
    "def beam_search():\n",
    "    NUM_MAX_DECODE = 8 #MAX_LEN\n",
    "    src_len_decode = src_len[0:1]\n",
    "    src_decode = src_tensor[0:1, :src_len_decode[0]]\n",
    "    init_token = trg_tensor[0:1, 0:1]\n",
    "    pred = sess.run(\n",
    "            transformer.decoder.outsoftmax.output[:,-1,:], {\n",
    "                transformer.wd_ind_src: src_decode,\n",
    "                transformer.input_lengths: src_len_decode,\n",
    "                transformer.wd_ind_trg: init_token,\n",
    "                transformer.output_lengths: np.ones(1)*init_token.shape[1],\n",
    "            })\n",
    "    n = 2\n",
    "    index = pred.argsort()[:,-n:]\n",
    "    probabilities = pred[:,index]\n",
    "    active_sentences = [np.concatenate((init_token, np.expand_dims(i, 0)), -1) for i in index.T]\n",
    "    active_probabilities = probabilities\n",
    "    for i in range(2):\n",
    "        active_sentences, active_probabilities = step_beam(active_sentences, active_probabilities)\n",
    "        print(active_probabilities, active_sentences)\n",
    "    translation = [EN.vocab.itos[a] for a in active_sentences[-1].flatten()]\n",
    "    print(translation)\n",
    "\n",
    "beam_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2 16]] 0.004180922\n",
      "[[ 2 15]] 0.99342126\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sum_prob = []\n",
    "sentences = []\n",
    "for choice, prob in zip(active_sentences, active_probabilities.ravel()):\n",
    "    print(choice, prob)\n",
    "    pred = sess.run(\n",
    "        transformer.decoder.outsoftmax.output[:,-1,:], {\n",
    "            transformer.wd_ind_src: src_decode,\n",
    "            transformer.input_lengths: src_len_decode,\n",
    "            transformer.wd_ind_trg: choice,\n",
    "            transformer.output_lengths: np.ones(1)*choice.shape[1],\n",
    "        })\n",
    "    index = pred.argsort()[:,-n:]\n",
    "    probabilities = pred[:,index]\n",
    "    sum_prob.append(probabilities + prob)\n",
    "    sub_sentences = [np.concatenate((choice, np.expand_dims(i, 0)), -1) for i in index.T]\n",
    "    sentences.append(sub_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_prob = np.concatenate([v.ravel() for v in sum_prob], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "sentences = list(itertools.chain.from_iterable(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_beam(active_sentences, active_probabilities):\n",
    "    sum_prob = []\n",
    "    sentences = []\n",
    "    for choice, prob in zip(active_sentences, active_probabilities.ravel()):\n",
    "#     print(choice, prob)\n",
    "        pred = sess.run(\n",
    "            transformer.decoder.outsoftmax.output[:,-1,:], {\n",
    "                transformer.wd_ind_src: src_decode,\n",
    "                transformer.input_lengths: src_len_decode,\n",
    "                transformer.wd_ind_trg: choice,\n",
    "                transformer.output_lengths: np.ones(1)*choice.shape[1],\n",
    "            })\n",
    "        index = pred.argsort()[:,-n:]\n",
    "        probabilities = pred[:,index]\n",
    "        sum_prob.append(probabilities + prob)\n",
    "        sub_sentences = [np.concatenate((choice, np.expand_dims(i, 0)), -1) for i in index.T]\n",
    "        sentences.append(sub_sentences)\n",
    "    sum_prob = np.concatenate([v.ravel() for v in sum_prob], 0)\n",
    "    sentences = list(itertools.chain.from_iterable(sentences))\n",
    "    index = sum_prob.argsort()[-n:]\n",
    "    return [sentences[i] for i in index], np.array([sum_prob[i] for i in index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 2, 16, 16]]),\n",
       " array([[ 2, 16, 15]]),\n",
       " array([[ 2, 15, 16]]),\n",
       " array([[ 2, 15, 15]])]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 16, 16,  2, 16, 15,  2, 15, 16,  2, 15, 15])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences\n",
    "np.concatenate([np.array(v).ravel() for v in senteces], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[0.00277754, 0.00556968, 0.99481004]]], dtype=float32),\n",
       " array([[[0.00556969, 0.00836184, 0.99760216]]], dtype=float32),\n",
       " array([[[0.99481004, 0.99760216, 1.9868425 ]]], dtype=float32)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 2, 12]]), array([[ 2, 16]]), array([[ 2, 15]])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0.00138879, 0.004181  , 0.99342114]]], dtype=float32),\n",
       " [array([[ 2, 12]]), array([[ 2, 16]]), array([[ 2, 15]])])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_probabilities, active_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 16 15]\n"
     ]
    }
   ],
   "source": [
    "for i in index:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(index[0,0], 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-b8b6bbf3b688>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "np.concatenate((init_token, np.expand_dims(index[0,0], 0)), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for dec_idx in range(NUM_MAX_DECODE):\n",
    "    pred = sess.run(\n",
    "        transformer.decoder.outsoftmax.output[:,-1,:], {\n",
    "            transformer.wd_ind_src: src_decode,\n",
    "            transformer.input_lengths: src_len_decode,\n",
    "            transformer.wd_ind_trg: autoregressive,\n",
    "            transformer.output_lengths: np.ones(1)*autoregressive.shape[1],\n",
    "        })\n",
    "    choice = pred.argmax(1)\n",
    "    autoregressive = np.concatenate((autoregressive, np.expand_dims(choice, 0)), -1)\n",
    "    if detect_end(choice, None):\n",
    "        break\n",
    "translation = [EN.vocab.itos[a] for a in autoregressive.flatten()]\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', 'sie', 'werden', 'auch', 'unter', 'hohen', 'temperaturen', 'betrieben', 'und', 'somit', 'kann', 'der', '<unk>', '<unk>', 'keine', '<unk>', 'erzeugen', ',', 'aber', 'falls']\n",
      "['<bos>', 'also', ',', 'they', 'operate']\n",
      "['<bos>', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_src(src_tensor, src_len, DE)\n",
    "print_src(trg_tensor, trg_len, EN)\n",
    "predict_one(src_tensor, src_len, trg_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "How how"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_MAX_DECODE = MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "src_len_decode = src_len[0:1]\n",
    "src_decode = src_tensor[0:1, :src_len_decode[0]]\n",
    "autoregressive = trg_tensor[0:1, 0:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dec_idx in range(10):\n",
    "    pred = sess.run(\n",
    "        transformer.decoder.outsoftmax.output[:,-1,:], {\n",
    "            transformer.wd_ind_src: src_decode,\n",
    "            transformer.input_lengths: src_len_decode,\n",
    "            transformer.wd_ind_trg: autoregressive,\n",
    "            transformer.output_lengths: np.ones(1)*autoregressive.shape[1],\n",
    "        })\n",
    "    choice = pred.argmax(1)\n",
    "    autoregressive = np.concatenate((autoregressive, np.expand_dims(choice, 0)), -1)\n",
    "    if detect_end(choice, None):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translation = [EN.vocab.itos[a] for a in autoregressive.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred[:,12:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
