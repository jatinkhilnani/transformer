{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Warning: no model found for 'de'\n",
      "\n",
      "    Only loading the 'de' tokenizer.\n",
      "\n",
      "\n",
      "    Warning: no model found for 'en'\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "# !pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib torchtext\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "# Standard PyTorch imports\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib.layers import layer_norm\n",
    "import nn_utils\n",
    "\n",
    "# For plots\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "import tensorflow as tf\n",
    "real = 1\n",
    "BATCH_SIZE = 64\n",
    "#!conda install torchtext spacy\n",
    "# !python -m spacy download en\n",
    "# !python -m spacy download de\n",
    "def detect_end(choice, eos_token=None):\n",
    "    return choice.flatten()[0] == eos_token\n",
    "\n",
    "if real:\n",
    "  from torchtext import data\n",
    "  from torchtext import datasets\n",
    "  import tqdm\n",
    "  import re\n",
    "  import spacy\n",
    "\n",
    "  spacy_de = spacy.load('de')\n",
    "  spacy_en = spacy.load('en')\n",
    "\n",
    "  url = re.compile('(<url>.*</url>)')\n",
    "\n",
    "  def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "  def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `spacey_en` not found.\n"
     ]
    }
   ],
   "source": [
    "?spacey_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Warning: no model found for 'en'\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  EN = data.Field(\n",
    "      tokenize=tokenize_en,\n",
    "      init_token='<bos>',\n",
    "      eos_token='<eos>',\n",
    "      include_lengths=True)\n",
    "\n",
    "  train, val, test = datasets.IWSLT.splits(\n",
    "      exts=('.de', '.en'), fields=(DE, EN))\n",
    "\n",
    "  train_it = data.Iterator(\n",
    "      train,\n",
    "      batch_size=BATCH_SIZE,\n",
    "      sort_within_batch=True,\n",
    "      train=True,\n",
    "      repeat=False,\n",
    "      shuffle=True)\n",
    "  MIN_WORD_FREQ = 10\n",
    "  MAX_NUM_WORDS = 10000\n",
    "  DE.build_vocab(train.src, min_freq=MIN_WORD_FREQ, max_size=MAX_NUM_WORDS)\n",
    "  EN.build_vocab(train.trg, min_freq=MIN_WORD_FREQ, max_size=MAX_NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "??train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in train.trg:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         \n"
     ]
    }
   ],
   "source": [
    "sentence = t\n",
    "tokens = parser(' '.join(sentence))\n",
    "lemmas = []\n",
    "for tok in tokens:\n",
    "    if not tok.is_punct:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "lemmatized_phrase = \"\"\n",
    "for l in lemmas:\n",
    "    lemmatized_phrase += l + \" \"\n",
    "lemmatized_phrase = lemmatized_phrase[:-1]\n",
    "print (lemmatized_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.lemma_ != \"-PRON-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i be    ', '     ', '   ', '    i be  ']\n"
     ]
    }
   ],
   "source": [
    "list = [\"I'm hoping to go jogging\", \"I haven't eaten in a while\",\"where is everybody going\", \n",
    "    \"Hello, how are you? I'm doing good.\"]\n",
    "lemmatized_list = []\n",
    "\n",
    "for sentence in list:\n",
    "    tokens = parser(sentence)\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        if not tok.is_punct:\n",
    "            lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    lemmatized_phrase = \"\"\n",
    "    for l in lemmas:\n",
    "        lemmatized_phrase += l + \" \"\n",
    "    lemmatized_phrase = lemmatized_phrase[:-1]\n",
    "    lemmatized_list.append(lemmatized_phrase)\n",
    "print (lemmatized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.lemma_.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tokens[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.is_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = 'This'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "David Gallo : This is Bill Lange . I 'm Dave Gallo ."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "David Gallo : This is Bill Lange . I 'm Dave Gallo ."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'         '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "# !pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib torchtext\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "# Standard PyTorch imports\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib.layers import layer_norm\n",
    "import nn_utils\n",
    "\n",
    "# For plots\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "import tensorflow as tf\n",
    "real = 1\n",
    "BATCH_SIZE = 64\n",
    "#!conda install torchtext spacy\n",
    "# !python -m spacy download en\n",
    "# !python -m spacy download de\n",
    "def detect_end(choice, eos_token=None):\n",
    "    return choice.flatten()[0] == eos_token\n",
    "\n",
    "if real:\n",
    "  from torchtext import data\n",
    "  from torchtext import datasets\n",
    "  import tqdm\n",
    "  import re\n",
    "  import spacy\n",
    "\n",
    "  spacy_de = spacy.load('de')\n",
    "  spacy_en = spacy.load('en')\n",
    "\n",
    "  url = re.compile('(<url>.*</url>)')\n",
    "\n",
    "  def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "  def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "  # Testing IWSLT\n",
    "  DE = data.Field(\n",
    "      tokenize=tokenize_de,\n",
    "      init_token='<bos>',\n",
    "      eos_token='<eos>',\n",
    "      include_lengths=True)\n",
    "  EN = data.Field(\n",
    "      tokenize=tokenize_en,\n",
    "      init_token='<bos>',\n",
    "      eos_token='<eos>',\n",
    "      include_lengths=True)\n",
    "\n",
    "  train, val, test = datasets.IWSLT.splits(\n",
    "      exts=('.de', '.en'), fields=(DE, EN))\n",
    "\n",
    "  train_it = data.Iterator(\n",
    "      train,\n",
    "      batch_size=BATCH_SIZE,\n",
    "      sort_within_batch=True,\n",
    "      train=True,\n",
    "      repeat=False,\n",
    "      shuffle=True)\n",
    "  MIN_WORD_FREQ = 10\n",
    "  MAX_NUM_WORDS = 10000\n",
    "  DE.build_vocab(train.src, min_freq=MIN_WORD_FREQ, max_size=MAX_NUM_WORDS)\n",
    "  EN.build_vocab(train.trg, min_freq=MIN_WORD_FREQ, max_size=MAX_NUM_WORDS)\n",
    "\n",
    "  num_wds_input = len(DE.vocab.itos)\n",
    "  num_wds_output = len(EN.vocab.itos)\n",
    "else:\n",
    "  num_wds_input = 1004\n",
    "\n",
    "\n",
    "class masked_softmax:\n",
    "  def __init__(self, v, mask, dim=2):\n",
    "    #bs, query dimension, key dimension\n",
    "    v_mask = v * mask\n",
    "    v_max = tf.reduce_max(v_mask, dim, keep_dims=True)\n",
    "    v_stable = v_mask - v_max\n",
    "\n",
    "    v_exp = tf.exp(v_stable) * mask\n",
    "    v_exp_sum = tf.reduce_sum(v_exp, dim, keep_dims=True)\n",
    "    self.v_mask, self.v_max, self.v_stable, self.v_exp, self.v_exp_sum = \\\n",
    "        v_mask, v_max, v_stable, v_exp, v_exp_sum\n",
    "    self.output = v_exp / (v_exp_sum + 1e-20)\n",
    "\n",
    "\n",
    "class Encoder:\n",
    "  def __init__(self, num_wds, wd_ind, mask, ndims=64, n_layers=6):\n",
    "    self.num_wds = num_wds\n",
    "    self.wd_ind = wd_ind\n",
    "    self.mask = mask\n",
    "    self.length = tf.shape(self.wd_ind)[1]\n",
    "    self.wd_emb = tf.Variable(\n",
    "        tf.random_uniform([self.num_wds, ndims], minval=-1, maxval=1.))\n",
    "    self.wd_vec = tf.nn.embedding_lookup(self.wd_emb, wd_ind)\n",
    "    self.pos = tf.reshape(\n",
    "        tf.range(tf.cast(self.length, tf.float32), dtype=tf.float32),\n",
    "        (1, -1, 1))\n",
    "    self.divider_exponent = tf.reshape(\n",
    "        tf.range(tf.cast(ndims // 2, tf.float32)),\n",
    "        (1, 1, -1)) * 2. / tf.cast(ndims, tf.float32)\n",
    "    self.divider = tf.pow(10000., self.divider_exponent)\n",
    "    self.input_to_sinusoids = self.pos / self.divider\n",
    "    self.pos_sin = tf.sin(self.input_to_sinusoids)\n",
    "    self.pos_cos = tf.cos(self.input_to_sinusoids)\n",
    "    # self.position = tf.reshape(\n",
    "    #     tf.range(tf.cast(self.length, tf.float32), dtype=tf.float32) / 10000,\n",
    "    #     (1, -1, 1))\n",
    "    self.position = tf.concat((self.pos_sin, self.pos_cos), -1)\n",
    "    self.w_tilde = embedding = self.wd_vec + self.position\n",
    "    self.encoding = []\n",
    "    self.attentionLayers = []\n",
    "    for _ in range(n_layers):\n",
    "      attentionLayer = AttentionLayer(embedding, mask)\n",
    "      embedding = attentionLayer.output\n",
    "      self.encoding.append(embedding)\n",
    "      self.attentionLayers.append(attentionLayer)\n",
    "\n",
    "\n",
    "class AttentionLayer:\n",
    "  def __init__(self, X, mask, X_decode=None, decode_mask=None, ff_layer=True):\n",
    "    bs, length, ndim = [v.value for v in X.shape]\n",
    "    self.X = X\n",
    "    if X_decode is None:\n",
    "      self.q, self.k, self.v = [\n",
    "          tf.tanh(tf.layers.dense(X, ndim)) for _ in range(3)\n",
    "      ]\n",
    "      decode_mask = mask\n",
    "    else:\n",
    "      self.q = tf.tanh(tf.layers.dense(X_decode, ndim))\n",
    "      self.k, self.v = [tf.tanh(tf.layers.dense(X, ndim)) for _ in range(2)]\n",
    "    #batch, attention queries, attention keys, embeddings\n",
    "    self.q_expanded = tf.expand_dims(self.q, 2)\n",
    "    self.k_expanded = tf.expand_dims(self.k, 1)\n",
    "    self.v_expanded = tf.expand_dims(self.v, 1)\n",
    "    self.s_raw = tf.reduce_sum(self.q_expanded * self.k_expanded, -1)\n",
    "    self.mask = tf.expand_dims(decode_mask, 2) * tf.expand_dims(mask, 1)\n",
    "    self.masked_softmax = masked_softmax(self.s_raw, self.mask)\n",
    "    self.s = self.masked_softmax.output\n",
    "    self.a = tf.expand_dims(self.s * self.mask, -1) * self.v_expanded\n",
    "    #A is shape bs, query, key, emb\n",
    "    self.a_compressed = tf.reduce_sum(self.a, 2)\n",
    "    if X_decode is None:\n",
    "      self.e = layer_norm(self.a_compressed + X)\n",
    "    else:\n",
    "      self.e = layer_norm(self.a_compressed + X_decode)\n",
    "    if ff_layer:\n",
    "      self.output = layer_norm(tf.layers.dense(self.e, ndim) + self.e)\n",
    "    else:\n",
    "      self.output = self.e\n",
    "\n",
    "\n",
    "class Decoder:\n",
    "  def __init__(self, num_wds, wd_ind, mask, encoder, ndims=20, n_layers=6):\n",
    "    self.num_wds = num_wds\n",
    "    self.wd_ind = wd_ind\n",
    "    self.mask = mask\n",
    "    self.encoder = encoder\n",
    "    self.length = tf.shape(self.wd_ind)[1]\n",
    "    self.wd_emb = tf.Variable(\n",
    "        tf.random_uniform([self.num_wds, ndims], minval=-1, maxval=1.))\n",
    "    self.wd_vec = tf.nn.embedding_lookup(self.wd_emb, wd_ind)\n",
    "    self.pos = tf.reshape(\n",
    "        tf.range(tf.cast(self.length, tf.float32), dtype=tf.float32),\n",
    "        (1, -1, 1))\n",
    "    self.divider_exponent = tf.reshape(\n",
    "        tf.range(tf.cast(ndims // 2, tf.float32)),\n",
    "        (1, 1, -1)) * 2. / tf.cast(ndims, tf.float32)\n",
    "    self.divider = tf.pow(10000., self.divider_exponent)\n",
    "    self.input_to_sinusoids = self.pos / self.divider\n",
    "    self.pos_sin = tf.sin(self.input_to_sinusoids)\n",
    "    self.pos_cos = tf.cos(self.input_to_sinusoids)\n",
    "    # self.position = tf.reshape(\n",
    "    #     tf.range(tf.cast(self.length, tf.float32), dtype=tf.float32) / 10000,\n",
    "    #     (1, -1, 1))\n",
    "    self.position = tf.concat((self.pos_sin, self.pos_cos), -1)\n",
    "    self.w_tilde = embedding = self.wd_vec + self.position\n",
    "    self.decoding = []\n",
    "    self.self_attentions = []\n",
    "    self.encoder_attentions = []\n",
    "    self.early_outputs = []\n",
    "    for l_idx in range(n_layers):\n",
    "      attn = AttentionLayer(embedding, mask, ff_layer=False)\n",
    "      self.self_attentions.append(attn)\n",
    "      encode_attn = AttentionLayer(encoder.encoding[l_idx], encoder.mask,\n",
    "                                   attn.output, mask)\n",
    "      self.encoder_attentions.append(encode_attn)\n",
    "      embedding = encode_attn.output\n",
    "      if l_idx < n_layers - 1:\n",
    "        early_output = tf.layers.dense(embedding, num_wds)\n",
    "        #early_output_masked = masked_softmax(early_output, tf.expand_dims(mask, -1), dim=2).output\n",
    "        self.early_outputs.append(early_output)\n",
    "\n",
    "    self.output_raw = tf.layers.dense(embedding, num_wds)\n",
    "    #bs, word in sentence of target, embedding\n",
    "\n",
    "    self.outsoftmax = masked_softmax(self.output_raw, tf.expand_dims(mask, -1), dim=2)\n",
    "    self.output = self.outsoftmax.output\n",
    "\n",
    "class Transformer:\n",
    "  def __init__(self, num_wds):\n",
    "    self.num_wds = num_wds\n",
    "    n_layers = 6\n",
    "    ndims = 256\n",
    "    self.learning_rate = tf.placeholder(tf.float32, None)\n",
    "    self.wd_ind_src = wd_ind_src = tf.placeholder(tf.int32, (None, None))\n",
    "    self.wd_ind_trg = wd_ind_trg = tf.placeholder(tf.int32, (None, None))\n",
    "    self.input_lengths = tf.placeholder(tf.int32, [None])\n",
    "    self.output_lengths = tf.placeholder(tf.int32, [None])\n",
    "    self.input_mask = tf.sequence_mask(\n",
    "        self.input_lengths,\n",
    "        maxlen=tf.shape(self.wd_ind_src)[-1],\n",
    "        dtype=tf.float32)\n",
    "    self.output_mask = tf.sequence_mask(\n",
    "        self.output_lengths,\n",
    "        maxlen=tf.shape(self.wd_ind_trg)[-1],\n",
    "        dtype=tf.float32)\n",
    "    self.encoder = Encoder(\n",
    "        num_wds, wd_ind_src, self.input_mask, n_layers=n_layers, ndims=ndims)\n",
    "    self.decoder = Decoder(\n",
    "        num_wds,\n",
    "        wd_ind_trg,\n",
    "        self.output_mask,\n",
    "        self.encoder,\n",
    "        n_layers=n_layers,\n",
    "        ndims=ndims)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "    self.prediction_mask = tf.concat((tf.zeros(\n",
    "        (tf.shape(self.output_mask)[0], 1)), self.output_mask[:, :-1] - self.output_mask[:, 1:]),\n",
    "                                     1)\n",
    "    self.losses = tf.reduce_mean([tf.reduce_mean(\n",
    "        tf.square(\n",
    "            tf.reduce_max(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=self.wd_ind_trg, logits=logits) *\n",
    "                self.prediction_mask, 1))) for logits in self.decoder.early_outputs + [self.decoder.output_raw]])\n",
    "    self.loss = tf.reduce_mean(\n",
    "        tf.square(\n",
    "            tf.reduce_max(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=self.wd_ind_trg, logits=self.decoder.output_raw) *\n",
    "                self.prediction_mask, 1)))\n",
    "    self.optimizer, self.grad_norm_total = nn_utils.apply_clipped_optimizer(\n",
    "        opt, self.losses)\n",
    "\n",
    "\n",
    "# In[71]:\n",
    "\n",
    "transformer = Transformer(num_wds_input)\n",
    "MAX_LEN = 20\n",
    "\n",
    "\n",
    "def predict_one(src_tensor, src_len, trg_tensor):\n",
    "    NUM_MAX_DECODE = 5 #MAX_LEN\n",
    "    src_len_decode = src_len[0:1]\n",
    "    src_decode = src_tensor[0:1, :src_len_decode[0]]\n",
    "    autoregressive = trg_tensor[0:1, 0:1]\n",
    "    for dec_idx in range(10):\n",
    "        pred = sess.run(\n",
    "            transformer.decoder.outsoftmax.output[:,-1,:], {\n",
    "                transformer.wd_ind_src: src_decode,\n",
    "                transformer.input_lengths: src_len_decode,\n",
    "                transformer.wd_ind_trg: autoregressive,\n",
    "                transformer.output_lengths: np.ones(1)*autoregressive.shape[1],\n",
    "            })\n",
    "        choice = pred.argmax(1)\n",
    "        autoregressive = np.concatenate((autoregressive, np.expand_dims(choice, 0)), -1)\n",
    "        if detect_end(choice, None):\n",
    "            break\n",
    "    translation = [EN.vocab.itos[a] for a in autoregressive.flatten()]\n",
    "    print(translation)\n",
    "def print_src(src_tensor, src_len, vocab):\n",
    "    src_len_decode = src_len[0:1]\n",
    "    src_decode = src_tensor[0:1, :src_len_decode[0]]\n",
    "    translation = [vocab.vocab.itos[a] for a in src_decode.flatten()]\n",
    "    print(translation)\n",
    "    \n",
    "    \n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "itr = 0\n",
    "print_freq = 100\n",
    "running_losses = []\n",
    "if real:\n",
    "  #for itr, train_batch in enumerate(tqdm.tqdm_notebook(train_it)):\n",
    "  for ep in range(100):\n",
    "      for train_batch in tqdm.tqdm_notebook(train_it):\n",
    "        itr += 1\n",
    "        #for itr in range(100000000):\n",
    "        src_tensor = train_batch.src[0].data.cpu().numpy().transpose()\n",
    "        src_len = train_batch.src[1].cpu().numpy()\n",
    "        trg_tensor = train_batch.trg[0].data.cpu().numpy().transpose()\n",
    "        trg_len = train_batch.trg[1].cpu().numpy()\n",
    "        src_tensor, trg_tensor = [t[:, :MAX_LEN] for t in [src_tensor, trg_tensor]]\n",
    "        src_len, trg_len = [np.clip(t, 0, MAX_LEN) for t in [src_len, trg_len]]\n",
    "        trg_len = np.ceil(\n",
    "            np.random.uniform(size=trg_len.shape[0]) * (trg_len - 1)).astype(int)\n",
    "        trn_feed_dict = {\n",
    "            transformer.wd_ind_src: src_tensor,\n",
    "            transformer.input_lengths: src_len,\n",
    "            transformer.wd_ind_trg: trg_tensor,\n",
    "            transformer.output_lengths: trg_len,\n",
    "            transformer.learning_rate: 1e-1 / (np.sqrt(itr + 3))\n",
    "        }\n",
    "        _, loss = sess.run([transformer.optimizer, transformer.loss],\n",
    "                           trn_feed_dict)\n",
    "        running_losses.append(loss)\n",
    "        if itr % print_freq == 0 or (itr < 5) or (itr < 100 and itr % 10 == 0):\n",
    "          running_losses = np.array(running_losses)\n",
    "          print(itr, 'loss_mean', running_losses.mean(), 'loss_std', running_losses.std(), 'loss_min', running_losses.min(),\n",
    "              'loss_max', running_losses.max())\n",
    "          running_losses = []\n",
    "          if itr % 1000 == 0:\n",
    "            print_src(src_tensor, src_len, DE)\n",
    "            print_src(trg_tensor, trg_len, EN)\n",
    "            predict_one(src_tensor, src_len, trg_tensor)\n",
    "        if itr > 2000000:\n",
    "          break\n",
    "else:\n",
    "\n",
    "  src_tensor = np.random.randint(low=0, high=num_wds_input, size=(BATCH_SIZE, 81))\n",
    "  src_len = np.random.randint(2, 81, BATCH_SIZE)\n",
    "  trg_tensor = np.random.randint(low=0, high=num_wds_input, size=(BATCH_SIZE, 84))\n",
    "  trg_len = np.random.randint(2, 84, BATCH_SIZE)\n",
    "\n",
    "  fd = {\n",
    "      transformer.wd_ind_src: src_tensor,\n",
    "      transformer.wd_ind_trg: trg_tensor,\n",
    "      transformer.input_lengths: src_len,\n",
    "      transformer.output_lengths: trg_len,\n",
    "      transformer.learning_rate: 1e-2}\n",
    "  sess.run([transformer.optimizer, transformer.loss], fd)\n",
    "# In[75]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MAX_DECODE = MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "src_len_decode = src_len[0:1]\n",
    "src_decode = src_tensor[0:1, :src_len_decode[0]]\n",
    "autoregressive = trg_tensor[0:1, 0:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dec_idx in range(10):\n",
    "    pred = sess.run(\n",
    "        transformer.decoder.outsoftmax.output[:,-1,:], {\n",
    "            transformer.wd_ind_src: src_decode,\n",
    "            transformer.input_lengths: src_len_decode,\n",
    "            transformer.wd_ind_trg: autoregressive,\n",
    "            transformer.output_lengths: np.ones(1)*autoregressive.shape[1],\n",
    "        })\n",
    "    choice = pred.argmax(1)\n",
    "    autoregressive = np.concatenate((autoregressive, np.expand_dims(choice, 0)), -1)\n",
    "    if detect_end(choice, None):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = [EN.vocab.itos[a] for a in autoregressive.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[:,12:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
